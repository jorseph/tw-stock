import pandas as pd
import logging
import os
import requests
import signal
import sys
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext
from dotenv import load_dotenv
from datetime import datetime, timedelta
import numpy as np
import asyncio
import json
import aiohttp
import time
from typing import Dict, List, Optional
import pickle

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,
    encoding='utf-8'  # æ·»åŠ  UTF-8 ç·¨ç¢¼
)
logger = logging.getLogger(__name__)

# å‰µå»ºä¸€å€‹å…¨å±€çš„ application è®Šé‡
app = None

# æ·»åŠ å…¨å±€è®Šé‡ä¾†è¿½è¹¤åŸ·è¡Œç‹€æ…‹
is_processing = False
current_task = None
should_cancel = False

# ç·©å­˜ç›¸é—œå¸¸é‡
CACHE_FILE = "stock_data_cache.pkl"
CACHE_EXPIRY_DAYS = 7  # æ”¹ç‚º 7 å¤©ï¼Œå› ç‚ºåŸºæœ¬é¢æ•¸æ“šè®ŠåŒ–è¼ƒæ…¢
BATCH_SIZE = 100  # å¢åŠ æ‰¹æ¬¡å¤§å°
DELAY_BETWEEN_BATCHES = 1  # æ¸›å°‘æ‰¹æ¬¡é–“å»¶é²åˆ° 30 ç§’
MAX_CONCURRENT_REQUESTS = 10  # å¢åŠ ä¸¦ç™¼è«‹æ±‚æ•¸

# ç·©å­˜æ•¸æ“šçµæ§‹
class StockDataCache:
    def __init__(self):
        self.data: Dict[str, Dict] = {}
        self.last_update: Dict[str, datetime] = {}
    
    def is_valid(self, stock_id: str) -> bool:
        if stock_id not in self.last_update:
            return False
        return (datetime.now() - self.last_update[stock_id]).days < CACHE_EXPIRY_DAYS
    
    def get(self, stock_id: str) -> Optional[Dict]:
        if self.is_valid(stock_id):
            return self.data.get(stock_id)
        return None
    
    def set(self, stock_id: str, data: Dict):
        self.data[stock_id] = data
        self.last_update[stock_id] = datetime.now()
    
    def save(self):
        with open(CACHE_FILE, 'wb') as f:
            pickle.dump(self, f)
    
    @classmethod
    def load(cls) -> 'StockDataCache':
        try:
            with open(CACHE_FILE, 'rb') as f:
                return pickle.load(f)
        except (FileNotFoundError, pickle.PickleError):
            return cls()

# å…¨å±€ç·©å­˜å°è±¡
stock_cache = StockDataCache.load()

# ä¿¡è™Ÿè™•ç†å‡½æ•¸
def signal_handler(signum, frame):
    logger.info("æ”¶åˆ°çµ‚æ­¢ä¿¡è™Ÿï¼Œæ­£åœ¨å„ªé›…é€€å‡º...")
    if app:
        logger.info("æ­£åœ¨åœæ­¢ Telegram Bot...")
        app.stop()
    sys.exit(0)

# print("ç•¶å‰å·¥ä½œç›®éŒ„:", os.getcwd())

import pandas as pd
from telegram import Update
from telegram.ext import CallbackContext

load_dotenv()
FINMIND_API_KEY = os.getenv("FINMIND_API_KEY")
FINMIND_URL = "https://api.finmindtrade.com/api/v4/data"

# è®€å–è‚¡ç¥¨åŸºæœ¬è³‡è¨Š CSV
CSV_FILE = "Calculated_Stock_Values.csv"
df = pd.read_csv(CSV_FILE)

# ç¢ºä¿ "ä»£è™Ÿ" æ¬„ä½ç‚ºå­—ä¸²
df["ä»£è™Ÿ"] = df["ä»£è™Ÿ"].astype(str)

# è®€å–é…æ¯è³‡è¨Š CSV
DIVIDEND_CSV_FILE = "all_stock_dividends.csv"
df_dividend = pd.read_csv(DIVIDEND_CSV_FILE)

# ç¢ºä¿ "stock_id" æ¬„ä½ç‚ºå­—ä¸²
df_dividend["stock_id"] = df_dividend["stock_id"].astype(str)

# ç¢ºä¿ "CashEarningsDistribution" æ¬„ä½æ˜¯æ•¸å€¼é¡å‹ï¼ˆé¿å… NaN å•é¡Œï¼‰
df_dividend["CashEarningsDistribution"] = pd.to_numeric(df_dividend["CashEarningsDistribution"], errors='coerce')

# è¨­å®šæ©Ÿå™¨äºº
async def start(update: Update, context: CallbackContext) -> None:
    await update.message.reply_text("æ­¡è¿ä½¿ç”¨è‚¡ç¥¨æŸ¥è©¢æ©Ÿå™¨äººï¼è«‹è¼¸å…¥ /stock <è‚¡ç¥¨ä»£è™Ÿ> æˆ– /recommend")


# Telegram Bot æŒ‡ä»¤ï¼š/stock_estimate 2330
async def stock_estimate(update: Update, context: CallbackContext) -> None:
    if not context.args:
        await update.message.reply_text("è«‹è¼¸å…¥è‚¡ç¥¨ä»£è™Ÿï¼Œä¾‹å¦‚ï¼š/stock_estimate 2330")
        return

    stock_id = context.args[0]
    df_result, now_price = await calculate_quarterly_stock_estimates(stock_id)

    if df_result is None:
        await update.message.reply_text(f"âš ï¸ ç„¡æ³•ç²å– {stock_id} çš„æ•¸æ“šï¼Œè«‹æª¢æŸ¥ API è¨­å®šæˆ–è‚¡ç¥¨ä»£è™Ÿ")
        return

    # è¨ˆç®—çµ±è¨ˆæ•¸æ“šçš„å¹³å‡å€¼
    avg_roe = df_result['ROE'].mean()
    avg_per_high = df_result['PER_æœ€é«˜å€¼'].mean()
    avg_per_normal = df_result['PER_å¹³å‡å€¼'].mean()
    avg_per_low = df_result['PER_æœ€ä½å€¼'].mean()

    # ä½¿ç”¨æœ€æ–°ä¸€ç­†çš„ BVPS å’Œå¹³å‡ ROE è¨ˆç®—æ¨ä¼° EPS
    latest_bvps = df_result.iloc[0]['BVPS']  # æœ€æ–°ä¸€ç­†çš„ BVPS
    estimated_eps = latest_bvps * (avg_roe / 100)  # ä½¿ç”¨å¹³å‡ ROE è¨ˆç®—

    # ä½¿ç”¨æ¨ä¼° EPS å’Œå¹³å‡ PER è¨ˆç®—è‚¡åƒ¹å€é–“
    low_price = estimated_eps * avg_per_low
    normal_price = estimated_eps * avg_per_normal
    high_price = estimated_eps * avg_per_high

    # ç”Ÿæˆå›æ‡‰è¨Šæ¯
    message = f"ğŸ“Š **{stock_id} å­£åº¦ ROE & æ¨ä¼°è‚¡åƒ¹** ğŸ“Š\n"
    message += f"\nğŸ”¹ **ç•¶å‰è‚¡åƒ¹**: {now_price:.2f} å…ƒ\n"
    
    # æ·»åŠ çµ±è¨ˆæ•¸æ“š
    message += f"\nğŸ“ˆ **çµ±è¨ˆæ•¸æ“šï¼ˆè¿‘20å­£å¹³å‡ï¼‰**:\n"
    message += f"ğŸ“Š **å¹³å‡ ROE**: {avg_roe:.2f}%\n"
    message += f"ğŸ“ˆ **å¹³å‡ PER å€é–“**: {avg_per_low:.2f} ~ {avg_per_normal:.2f} ~ {avg_per_high:.2f}\n"
    message += f"ğŸ’° **æ¨ä¼°EPS**: {estimated_eps:.2f} å…ƒ (ä½¿ç”¨æœ€æ–° BVPS: {latest_bvps:.2f} Ã— å¹³å‡ ROE: {avg_roe:.2f}%)\n"
    message += f"ğŸ“‰ **æ¨ä¼°è‚¡åƒ¹å€é–“**: {low_price:.2f} ~ {normal_price:.2f} ~ {high_price:.2f} å…ƒ\n"

    await update.message.reply_text(message, parse_mode="Markdown")


async def etf(update: Update, context: CallbackContext) -> None:
    if not context.args:
        await update.message.reply_text("è«‹è¼¸å…¥ ETF ä»£è™Ÿï¼Œä¾‹å¦‚ï¼š/etf 00713")
        return
    
    # ğŸ”¹ æŸ¥è©¢ç•¶å‰è‚¡åƒ¹
    stock_id = context.args[0]
    current_price = await get_current_stock_price(stock_id)

    if current_price is None:
        await update.message.reply_text(f"ç„¡æ³•ç²å– {stock_id} çš„æœ€æ–°è‚¡åƒ¹ï¼Œè«‹ç¨å¾Œå†è©¦")
        return

    # ğŸ”¹ è¨ˆç®—æœ€è¿‘ä¸€å¹´é…æ¯ç¸½é¡ & æ®–åˆ©ç‡
    # total_dividends, dividend_yield = calculate_dividend_yield(stock_id, current_price)
    total_dividends, dividend_yield, dividends_count = calculate_all_dividend_yield(stock_id, current_price)

    # ğŸ”¹ å›æ‡‰è¨Šæ¯
    message = (
        f"ğŸ“Š **ETF è³‡è¨Š - {stock_id}**\n"
        f"ğŸ”¹ **ç•¶å‰è‚¡åƒ¹**: {current_price:.2f} å…ƒ\n"
        f"ğŸ’¸ **æœ€è¿‘ä¸€å¹´é…æ¯ç¸½é¡**: {total_dividends:.2f} å…ƒ ğŸ’°\n"
        f"ğŸ“Š **æ®–åˆ©ç‡**: {dividend_yield:.2f}%\n"
        f"ğŸ”¹ **é…æ¯ç­†æ•¸**: {dividends_count} ç­†\n"
    )
    
    await update.message.reply_text(message, parse_mode="Markdown")


async def get_stock_price_from_date(stock_id, query_date):
    """
    æ ¹æ“šæŒ‡å®šçš„ query_date (YYYY-MM-DD) æŸ¥è©¢è©²æ—¥æœŸè‡³ä»Šçš„è‚¡ç¥¨æ¯æ—¥åƒ¹æ ¼è³‡æ–™ï¼Œ
    ä¸¦åªå›å‚³ "date", "stock_id" èˆ‡ "close" ä¸‰å€‹æ¬„ä½ã€‚
    """
    check_date = query_date  # ä½¿ç”¨æŒ‡å®šæ—¥æœŸ

    date_str = check_date.strftime('%Y-%m-%d')
    parameter = {
        "dataset": "TaiwanStockPrice",
        "data_id": stock_id,
        "start_date": date_str,
        "token": FINMIND_API_KEY,
    }
    async with aiohttp.ClientSession() as session:
        async with session.get(FINMIND_URL, params=parameter) as response:
            if response.status != 200:
                logger.error(f"API è¯·æ±‚å¤±è´¥ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status}")
                return None
            data = await response.json()
            if "data" in data and isinstance(data["data"], list) and len(data["data"]) > 0:
                df_price = pd.DataFrame(data["data"])
                # åªä¿ç•™ date, stock_id, close æ¬„ä½
                required_cols = ["date", "stock_id", "close"]
                # ç¢ºä¿æ‰€æœ‰éœ€è¦çš„æ¬„ä½å­˜åœ¨
                for col in required_cols:
                    if col not in df_price.columns:
                        logger.error(f"æ•¸æ“šä¸­ç¼ºå°‘å¿…è¦æ¬„ä½: {col}")
                        return None
                df_price = df_price[required_cols]
                return df_price
        return None





# ä¿®æ”¹ get_current_stock_price å‡½æ•¸ç‚ºç•°æ­¥å‡½æ•¸
async def get_current_stock_price(stock_id):
    """è·å–è‚¡ç¥¨å½“å‰ä»·æ ¼ï¼Œç›´æ¥ä» API è·å–æœ€è¿‘5å¤©çš„æ•°æ®"""
    try:
        # è·å–æœ€è¿‘5å¤©çš„æ•°æ®
        parameter = {
            "dataset": "TaiwanStockPrice",
            "start_date": (datetime.today() - timedelta(days=5)).strftime('%Y-%m-%d'),
            "token": FINMIND_API_KEY,
        }

        async with aiohttp.ClientSession() as session:
            async with session.get(FINMIND_URL, params=parameter) as response:
                if response.status != 200:
                    logger.error(f"API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status}")
                    return None

                data = await response.json()

                # æ£€æŸ¥ API å›åº”æ˜¯å¦æœ‰æ•°æ®
                if "data" in data and isinstance(data["data"], list) and len(data["data"]) > 0:
                    df_price = pd.DataFrame(data["data"])
                    
                    # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
                    df_price['date'] = pd.to_datetime(df_price['date'])
                    
                    # ç¡®ä¿æ•°å€¼å­—æ®µä¸ºæ•°å€¼ç±»å‹
                    numeric_columns = ['Trading_Volume', 'Trading_money', 'open', 'max', 'min', 'close', 'spread', 'Trading_turnover']
                    for col in numeric_columns:
                        if col in df_price.columns:
                            df_price[col] = pd.to_numeric(df_price[col], errors='coerce')
                    
                    # è·å–æŒ‡å®šè‚¡ç¥¨çš„æœ€æ–°æ”¶ç›˜ä»·
                    stock_data = df_price[df_price['stock_id'] == stock_id]
                    if not stock_data.empty:
                        latest_price = stock_data.sort_values('date').iloc[-1]['close']
                        logger.info(f"æˆåŠŸè·å–è‚¡ç¥¨ {stock_id} çš„æœ€æ–°ä»·æ ¼ï¼š{latest_price}")
                        return latest_price
                    else:
                        logger.warning(f"æœªæ‰¾åˆ°è‚¡ç¥¨ {stock_id} çš„ä»·æ ¼æ•°æ®")
                else:
                    logger.warning("API è¿”å›æ•°æ®ä¸ºç©º")

        return None

    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨ {stock_id} ä»·æ ¼æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return None
    

def calculate_dividend_yield(stock_id, current_price):
    """ è¨ˆç®—è©² ETF æˆ–è‚¡ç¥¨çš„æœ€è¿‘ä¸€å¹´åº¦é…æ¯ç¸½é¡ï¼Œä¸¦è¨ˆç®—æ®–åˆ©ç‡ """
    
    # éæ¿¾ç‰¹å®šè‚¡ç¥¨
    stock_dividends = df_dividend[(df_dividend["stock_id"] == stock_id) & (df_dividend["CashEarningsDistribution"] > 0)].copy()

    # ç¢ºä¿ date æ¬„ä½æ˜¯ datetime æ ¼å¼
    stock_dividends["date"] = pd.to_datetime(stock_dividends["date"], errors="coerce", infer_datetime_format=True)

    if stock_dividends.empty:
        return 0.0, 0.0  # å¦‚æœè©²è‚¡ç¥¨ç„¡é…æ¯è³‡æ–™ï¼Œå‰‡å›å‚³ 0

    # ğŸ”¹ å–å¾—æœ€è¿‘ä¸€å¹´çš„é…æ¯
    one_year_ago = datetime.today() - timedelta(days=365)
    
    # **é€™è¡ŒéŒ¯èª¤çš„æ¯”è¼ƒæ”¹ç‚ºç¢ºä¿ date æ¬„ä½æ˜¯ datetime**
    last_year_dividends = stock_dividends[stock_dividends["date"] >= one_year_ago]

    # è¨ˆç®—å¹´åº¦é…æ¯ç¸½é¡
    total_dividends = last_year_dividends["CashEarningsDistribution"].sum()

    # è¨ˆç®—æ®–åˆ©ç‡
    if current_price > 0:
        dividend_yield = (total_dividends / current_price) * 100
    else:
        dividend_yield = 0.0

    return total_dividends, dividend_yield


# ğŸ”¹ æŸ¥è©¢é…æ¯è³‡æ–™ä¸¦è¨ˆç®—å®Œæ•´æ®–åˆ©ç‡
def calculate_all_dividend_yield(stock_id, current_price):
    """ 
    è¨ˆç®—å®Œæ•´æ®–åˆ©ç‡ï¼ˆåŒ…å«ç¾é‡‘èˆ‡è‚¡ç¥¨è‚¡åˆ©ï¼‰ 
    """
    # ğŸ”¹ éæ¿¾è©²è‚¡ç¥¨çš„é…æ¯è³‡æ–™
    stock_dividends = df_dividend[df_dividend["stock_id"] == stock_id].copy()

    # ç¢ºä¿ date æ¬„ä½æ˜¯ datetime æ ¼å¼
    stock_dividends["date"] = pd.to_datetime(stock_dividends["date"], errors="coerce", infer_datetime_format=True)
    
    if stock_dividends.empty:
        return 0.0, 0.0, 0  # å¦‚æœè©²è‚¡ç¥¨ç„¡é…æ¯è³‡æ–™ï¼Œå‰‡å›å‚³ 0

    # å…ˆæŒ‰ç…§æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    stock_dividends = stock_dividends.sort_values(by="date", ascending=False)

    # å–å¾—æœ€è¿‘ä¸€å¹´çš„é…æ¯
    one_year_ago = datetime.today() - timedelta(days=365)
    today = datetime.today()
    
    # å–å¾—æœ€è¿‘ä¸€å¹´çš„é…æ¯è³‡æ–™ï¼Œä¸¦ç¢ºä¿ä¸é‡è¤‡
    last_year_dividends = stock_dividends[
        (stock_dividends["date"] >= one_year_ago) & 
        (stock_dividends["date"] <= today) &  # æ’é™¤æœªä¾†çš„é…æ¯æ—¥æœŸ
        (stock_dividends["CashEarningsDistribution"] > 0)  # åªå–æœ‰ç¾é‡‘è‚¡åˆ©çš„è³‡æ–™
    ].drop_duplicates(subset=["date"])  # ç§»é™¤åŒä¸€å¤©çš„é‡è¤‡è³‡æ–™
    
    # ç¢ºä¿è‡³å°‘æœ‰ 1 ç­†é…æ¯è³‡æ–™
    if last_year_dividends.empty:
        return 0.0, 0.0, 0

    # è¨ˆç®—æœ€è¿‘ä¸€å¹´çš„ **ç¾é‡‘è‚¡åˆ©ç¸½é¡**
    total_cash_dividends = last_year_dividends["CashEarningsDistribution"].sum()

    # è¨ˆç®—æœ€è¿‘ä¸€å¹´çš„ **è‚¡ç¥¨è‚¡åˆ©ç¸½é¡**
    total_stock_dividends = last_year_dividends["StockEarningsDistribution"].sum()

    # **è¨ˆç®—é™¤æ¬Šæ¯å¾Œè‚¡åƒ¹**
    ex_rights_price = max(current_price - total_cash_dividends, 0)  # ç¢ºä¿è‚¡åƒ¹ä¸ç‚ºè² 

    # **è¨ˆç®—è‚¡ç¥¨è‚¡åˆ©åƒ¹å€¼**
    stock_dividend_value = total_stock_dividends * ex_rights_price / 1000

    # **è¨ˆç®—ç¸½è‚¡åˆ©åƒ¹å€¼**
    total_dividend_value = stock_dividend_value + (total_cash_dividends)

    # **è¨ˆç®—é‚„åŸæ®–åˆ©ç‡**
    if current_price > 0:
        restored_dividend_yield = (total_dividend_value / current_price) * 100.00
    else:
        restored_dividend_yield = 0.0

    return total_dividend_value, restored_dividend_yield, len(last_year_dividends)


# ä¿®æ”¹ calculate_quarterly_stock_estimates å‡½æ•¸ç‚ºç•°æ­¥å‡½æ•¸
async def calculate_quarterly_stock_estimates(stock_id, start_date="2020-01-01", end_date=None):
    """ å¾ CSV æ–‡ä»¶è®€å–æ•¸æ“šï¼Œè¨ˆç®—å­£åº¦ ROEã€BVPSã€æ¨ä¼°è‚¡åƒ¹ """
    try:
        # è®€å– CSV æ–‡ä»¶
        csv_file = "stock_roe_data.csv"
        if not os.path.exists(csv_file):
            logger.error(f"æ‰¾ä¸åˆ° {csv_file} æ–‡ä»¶")
            return None

        # è®€å–æ•¸æ“šä¸¦éæ¿¾æŒ‡å®šè‚¡ç¥¨
        df = pd.read_csv(csv_file)
        df['stock_id'] = df['stock_id'].astype(str)
        df = df[df['stock_id'] == stock_id]

        if df.empty:
            logger.warning(f"è‚¡ç¥¨ {stock_id} åœ¨ CSV ä¸­æ²’æœ‰æ•¸æ“š")
            return None

        # ç¢ºä¿æ—¥æœŸæ ¼å¼æ­£ç¢º
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        
        # ç¢ºä¿æ•¸å€¼æ¬„ä½ç‚ºæ•¸å€¼é¡å‹
        numeric_columns = ["PER", "PBR"]
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")

        # éæ¿¾ç„¡æ•ˆçš„ PER å’Œ PBR æ•¸æ“š
        df = df[
            (df['PER'] > 0) & (df['PER'] < 100) &  # åˆç†çš„ PER ç¯„åœ
            (df['PBR'] > 0) & (df['PBR'] < 10)     # åˆç†çš„ PBR ç¯„åœ
        ]

        if df.empty:
            logger.warning(f"è‚¡ç¥¨ {stock_id} æ²’æœ‰æœ‰æ•ˆçš„ PER å’Œ PBR æ•¸æ“š")
            return None

        # ä¾å­£åº¦å–æ•¸æ“š
        df["quarter"] = df["date"].dt.to_period("Q")
        
        # è¨ˆç®—å­£åº¦ PER çµ±è¨ˆæ•¸æ“šï¼ˆä½¿ç”¨ç™¾åˆ†ä½æ•¸é¿å…æ¥µç«¯å€¼ï¼‰
        df_per_stats = df.groupby("quarter").agg(
            PER_æœ€é«˜å€¼=("PER", lambda x: np.percentile(x, 95)),  # 95th percentile
            PER_å¹³å‡å€¼=("PER", "mean"),
            PER_æœ€ä½å€¼=("PER", lambda x: np.percentile(x, 5))    # 5th percentile
        ).reset_index()

        # è¨ˆç®—æ¯å­£åº¦çš„å¹³å‡å€¼
        df_quarterly = df.groupby("quarter").agg(
            date=("date", "last"),
            PER=("PER", "mean"),
            PBR=("PBR", "median"),
            close=("close", "last")
        ).reset_index()

        # åˆä½µ PER çµ±è¨ˆæ•¸æ“š
        df_quarterly = df_quarterly.merge(df_per_stats, on="quarter", how="left")

        # è¨ˆç®—å­£åº¦ ROE (%)
        df_quarterly["ROE"] = np.where(
            (df_quarterly['PER'] != 0) & (df_quarterly['PER'].notna()) & (df_quarterly['PBR'].notna()),
            (df_quarterly['PBR'] / df_quarterly['PER']) * 100,
            np.nan
        )

        # æ ¹æ“šæ¯å€‹å­£åº¦çš„æœ€å¾Œæ—¥æœŸå¾ CSV ä¸­ç²å–ç•¶å¤©æ”¶ç›¤åƒ¹ï¼Œä½œç‚º prev_close
        prev_closes = []
        for _, row in df_quarterly.iterrows():
            quarter_end_date = row["date"]  # è©²å­£åº¦æœ€å¾Œä¸€å¤©çš„æ—¥æœŸ
            # å¾åŸå§‹æ•¸æ“šä¸­ç²å–è©²æ—¥æœŸçš„æ”¶ç›¤åƒ¹
            price = df[df["date"] == quarter_end_date]["close"].iloc[0] if not df[df["date"] == quarter_end_date].empty else np.nan
            if price is None or price == 0:
                logger.warning(f"è‚¡ç¥¨ {stock_id} åœ¨ {quarter_end_date.strftime('%Y-%m-%d')} çš„æ”¶ç›¤åƒ¹ç„¡æ•ˆ")
                price = np.nan
            prev_closes.append(price)
        df_quarterly["prev_close"] = prev_closes

        # æ–°å¢æª¢æŸ¥ï¼šå¦‚æœ prev_close ç‚º 0 æˆ– NaNï¼Œå‰‡éæ¿¾æ‰é€™äº›è¡Œ
        invalid_count = df_quarterly[(df_quarterly["prev_close"] == 0) | (df_quarterly["prev_close"].isna())].shape[0]
        if invalid_count > 0:
            logger.warning(f"è‚¡ç¥¨ {stock_id} æœ‰ {invalid_count} è¡Œå­£åº¦æ•¸æ“šçš„ prev_close ç‚º 0 æˆ–ç„¡æ•ˆï¼Œå°‡éæ¿¾æ‰é€™äº›æ•¸æ“š")
            return None
        df_quarterly = df_quarterly[(df_quarterly["prev_close"] != 0) & (df_quarterly["prev_close"].notna())]

        df_quarterly["BVPS"] = df_quarterly["prev_close"] / df_quarterly["PBR"]

        # è¨ˆç®—æ¨ä¼°EPS
        df_quarterly["æ¨ä¼°EPS"] = df_quarterly["BVPS"] * (df_quarterly["ROE"] / 100)

        # è¨ˆç®—ä¸‰ç¨®è‚¡åƒ¹ï¼ˆä½¿ç”¨ PER çš„ç™¾åˆ†ä½æ•¸ï¼‰
        df_quarterly["é«˜è‚¡åƒ¹"] = df_quarterly["PER_æœ€é«˜å€¼"] * df_quarterly["æ¨ä¼°EPS"]
        df_quarterly["æ­£å¸¸è‚¡åƒ¹"] = df_quarterly["PER_å¹³å‡å€¼"] * df_quarterly["æ¨ä¼°EPS"]
        df_quarterly["ä½è‚¡åƒ¹"] = df_quarterly["PER_æœ€ä½å€¼"] * df_quarterly["æ¨ä¼°EPS"]

        # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        df_quarterly = df_quarterly.sort_values("date", ascending=False)

        # ç§»é™¤ç„¡æ•ˆçš„ä¼°å€¼
        df_quarterly = df_quarterly[
            df_quarterly[["ROE", "BVPS", "æ¨ä¼°EPS", "é«˜è‚¡åƒ¹", "æ­£å¸¸è‚¡åƒ¹", "ä½è‚¡åƒ¹"]].notna().all(axis=1)
        ]

        if df_quarterly.empty:
            logger.warning(f"è‚¡ç¥¨ {stock_id} ç„¡æœ‰æ•ˆçš„å­£åº¦æ•¸æ“š")
            return None

        # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„å­£åº¦æ•¸æ“š
        if len(df_quarterly) < 16:
            logger.warning(f"è‚¡ç¥¨ {stock_id} çš„å­£åº¦æ•¸æ“šä¸è¶³ 4 å¹´")
            return None

        # æª¢æŸ¥æœ€æ–°æ•¸æ“šæ˜¯å¦åœ¨æœ€è¿‘ä¸€å¹´å…§
        latest_date = df_quarterly.iloc[0]["date"]
        one_year_ago = pd.Timestamp.now() - pd.DateOffset(years=1)
        
        if latest_date < one_year_ago:
            logger.warning(f"è‚¡ç¥¨ {stock_id} çš„æœ€æ–°æ•¸æ“šéæœŸï¼ˆ{latest_date.strftime('%Y-%m-%d')}ï¼‰")
            return None

        # æª¢æŸ¥æ˜¯å¦å¾2020å¹´è‡³ä»Šçš„æ¯ä¸€å­£éƒ½æœ‰æ•¸æ“š
        start_date = pd.Timestamp("2020-01-01")
        end_date = pd.Timestamp.now()
        
        # ç”Ÿæˆæ‰€æœ‰æ‡‰è©²æœ‰çš„å­£åº¦
        all_quarters = pd.period_range(start=start_date, end=end_date, freq='Q')
        
        # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰å­£åº¦éƒ½æœ‰æ•¸æ“šï¼ˆé™¤äº†ç•¶å­£ï¼‰
        existing_quarters = set(df_quarterly['quarter'])
        current_quarter = pd.Timestamp.now().to_period('Q')
        missing_quarters = [q for q in all_quarters if q not in existing_quarters and q < current_quarter]
        
        if missing_quarters:
            logger.warning(f"è‚¡ç¥¨ {stock_id} ç¼ºå°‘ä»¥ä¸‹å­£åº¦çš„æ•¸æ“š: {missing_quarters}")
            return None

        # å°‡è¨ˆç®—çµæœæ“´å……åˆ°åŸå§‹æ•¸æ“šä¸­
        try:
            # è®€å–åŸå§‹ CSV æ–‡ä»¶
            df_original = pd.read_csv(csv_file)
            df_original['stock_id'] = df_original['stock_id'].astype(str)
            
            # ç‚ºæ¯å€‹æ—¥æœŸæ·»åŠ è¨ˆç®—çµæœ
            for _, row in df_quarterly.iterrows():
                mask = (df_original['stock_id'] == stock_id) & (df_original['date'] == row['date'])
                if any(mask):
                    # æ›´æ–°ç¾æœ‰è¡Œ
                    df_original.loc[mask, 'ROE'] = row['ROE']
                    df_original.loc[mask, 'BVPS'] = row['BVPS']
                    df_original.loc[mask, 'æ¨ä¼°EPS'] = row['æ¨ä¼°EPS']
                    df_original.loc[mask, 'é«˜è‚¡åƒ¹'] = row['é«˜è‚¡åƒ¹']
                    df_original.loc[mask, 'æ­£å¸¸è‚¡åƒ¹'] = row['æ­£å¸¸è‚¡åƒ¹']
                    df_original.loc[mask, 'ä½è‚¡åƒ¹'] = row['ä½è‚¡åƒ¹']
                else:
                    # æ·»åŠ æ–°è¡Œ
                    new_row = pd.DataFrame([{
                        'stock_id': stock_id,
                        'date': row['date'],
                        'ROE': row['ROE'],
                        'BVPS': row['BVPS'],
                        'æ¨ä¼°EPS': row['æ¨ä¼°EPS'],
                        'é«˜è‚¡åƒ¹': row['é«˜è‚¡åƒ¹'],
                        'æ­£å¸¸è‚¡åƒ¹': row['æ­£å¸¸è‚¡åƒ¹'],
                        'ä½è‚¡åƒ¹': row['ä½è‚¡åƒ¹'],
                        'PER': row['PER'],
                        'PBR': row['PBR'],
                        'PER_æœ€é«˜å€¼': row['PER_æœ€é«˜å€¼'],
                        'PER_å¹³å‡å€¼': row['PER_å¹³å‡å€¼'],
                        'PER_æœ€ä½å€¼': row['PER_æœ€ä½å€¼']
                    }])
                    df_original = pd.concat([df_original, new_row], ignore_index=True)

            # ä¿å­˜æ›´æ–°å¾Œçš„æ•¸æ“š
            df_original.to_csv(csv_file, index=False, encoding='utf-8-sig')
            logger.info(f"å·²å°‡è‚¡ç¥¨ {stock_id} çš„è¨ˆç®—çµæœå¯«å…¥ CSV æ–‡ä»¶")
        except Exception as e:
            logger.error(f"å¯«å…¥ CSV æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

        # è¿”å›è¨ˆç®—çµæœå’Œç•¶å‰è‚¡åƒ¹
        return df_quarterly, price

    except Exception as e:
        logger.error(f"è™•ç†è‚¡ç¥¨ {stock_id} æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return None

# æ·»åŠ ç²å–å°è‚¡ä»£è™Ÿåˆ—è¡¨çš„å‡½æ•¸
def get_taiwan_stock_list():
    """å¾  FinMind API ç²å–å°è‚¡ä»£è™Ÿåˆ—è¡¨"""
    parameter = {
        "dataset": "TaiwanStockInfo",
        "token": FINMIND_API_KEY,
    }

    try:
        response = requests.get(FINMIND_URL, params=parameter)
        data = response.json()

        if "data" not in data or not isinstance(data["data"], list):
            logger.error("ç„¡æ³•å¾ FinMind API ç²å–è‚¡ç¥¨åˆ—è¡¨")
            return []

        # è½‰æ›ç‚º DataFrame
        df_stocks = pd.DataFrame(data["data"])
        
        # ç¢ºä¿ stock_id æ¬„ä½ç‚ºå­—ä¸²
        df_stocks["stock_id"] = df_stocks["stock_id"].astype(str)
        
        # éæ¿¾æ‰éä¸Šå¸‚è‚¡ç¥¨ï¼ˆé€šå¸¸è‚¡ç¥¨ä»£ç¢¼é•·åº¦ç‚º 4 ä½ï¼‰
        df_stocks = df_stocks[df_stocks["stock_id"].str.len() == 4]

        # éæ¿¾æ‰ç‰¹æ®Šè‚¡ç¥¨ï¼ˆå¦‚æ¬Šè­‰ã€æœŸè²¨ç­‰ï¼‰
        df_stocks = df_stocks[~df_stocks["stock_id"].str.startswith(('0', '9'))]

        
        # æ·»åŠ æ—¥èªŒè¨˜éŒ„
        logger.info(f"å¾ API ç²å–çš„è‚¡ç¥¨ç¸½æ•¸ï¼š{len(df_stocks)}")
        logger.info(f"è‚¡ç¥¨ä»£ç¢¼ç¯„åœï¼š{df_stocks['stock_id'].min()} åˆ° {df_stocks['stock_id'].max()}")
        
        stock_list = df_stocks["stock_id"].tolist()
        logger.info(f"æˆåŠŸç²å– {len(stock_list)} æ”¯ä¸Šå¸‚è‚¡ç¥¨")
        return stock_list
        
    except Exception as e:
        logger.error(f"ç²å–è‚¡ç¥¨åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return []

# æ·»åŠ æ–·é»çºŒå‚³ç›¸é—œè®Šé‡
progress_file = "recommend_v2_progress.json"

def save_progress(stock_list, current_index):
    """ä¿å­˜ä¸‹è¼‰é€²åº¦"""
    try:
        progress_data = {
            'stock_list': stock_list,
            'current_index': current_index,
            'timestamp': datetime.now().isoformat(),
            'total_stocks': len(stock_list)
        }
        with open(progress_file, 'w') as f:
            json.dump(progress_data, f)
        logger.info(f"å·²ä¿å­˜é€²åº¦ï¼šç•¶å‰è™•ç†åˆ°ç¬¬ {current_index} ç­†ï¼Œå…± {len(stock_list)} ç­†")
    except Exception as e:
        logger.error(f"ä¿å­˜ä¸‹è¼‰é€²åº¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

def load_progress():
    """è¼‰å…¥é€²åº¦"""
    try:
        if os.path.exists(progress_file):
            with open(progress_file, 'r') as f:
                data = json.load(f)
                # æª¢æŸ¥é€²åº¦æ˜¯å¦éæœŸï¼ˆè¶…é 24 å°æ™‚ï¼‰
                if datetime.fromisoformat(data['timestamp']) + timedelta(hours=24) < datetime.now():
                    logger.info("ä¸‹è¼‰é€²åº¦å·²éæœŸï¼Œå°‡é‡æ–°é–‹å§‹")
                    return None, 0
                
                # é©—è­‰æ•¸æ“šå®Œæ•´æ€§
                if 'stock_list' not in data or 'current_index' not in data or 'total_stocks' not in data:
                    logger.error("ä¸‹è¼‰é€²åº¦æª”æ¡ˆæ ¼å¼ä¸æ­£ç¢º")
                    return None, 0
                
                logger.info(f"è¼‰å…¥ä¸‹è¼‰é€²åº¦ï¼šå¾ç¬¬ {data['current_index']} ç­†é–‹å§‹ï¼Œå…± {data['total_stocks']} ç­†")
                return data['stock_list'], data['current_index']
    except Exception as e:
        logger.error(f"è¼‰å…¥ä¸‹è¼‰é€²åº¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    return None, 0

# æ·»åŠ å–æ¶ˆå‘½ä»¤
async def cancel_recommend(update: Update, context: CallbackContext) -> None:
    """å–æ¶ˆæ­£åœ¨åŸ·è¡Œçš„æ¨è–¦ä»»å‹™"""
    global is_processing, should_cancel
    
    if not is_processing:
        await update.message.reply_text("ç›®å‰æ²’æœ‰æ­£åœ¨åŸ·è¡Œçš„æ¨è–¦ä»»å‹™")
        return
    
    should_cancel = True
    is_processing = False
    await update.message.reply_text("å·²ç™¼é€å–æ¶ˆæŒ‡ä»¤ï¼Œæ­£åœ¨ç­‰å¾…ä»»å‹™çµæŸ...")

async def recommend_v2(update: Update, context: CallbackContext) -> None:
    """æ¨è–¦è‚¡ç¥¨ v2 ç‰ˆæœ¬"""
    global is_processing, should_cancel
    
    if is_processing:
        logger.warning("å·²æœ‰æ¨è–¦ä»»å‹™æ­£åœ¨åŸ·è¡Œä¸­")
        await update.message.reply_text("å·²æœ‰æ¨è–¦ä»»å‹™æ­£åœ¨åŸ·è¡Œä¸­ï¼Œè«‹ç¨å¾Œå†è©¦")
        return
    
    try:
        is_processing = True
        should_cancel = False
        logger.info("é–‹å§‹åŸ·è¡Œè‚¡ç¥¨æ¨è–¦ä»»å‹™")
        
        # è®€å–è‚¡åƒ¹æ•¸æ“š
        if not os.path.exists(STOCK_PRICE_FILE):
            logger.error(f"æ‰¾ä¸åˆ°è‚¡åƒ¹æ•¸æ“šæ–‡ä»¶ï¼š{STOCK_PRICE_FILE}")
            await update.message.reply_text("æ‰¾ä¸åˆ°è‚¡åƒ¹æ•¸æ“šæ–‡ä»¶ï¼Œè«‹å…ˆåŸ·è¡Œ /sync_stock_prices å‘½ä»¤")
            return

        logger.info("é–‹å§‹è®€å–è‚¡åƒ¹æ•¸æ“šæ–‡ä»¶")
        with open(STOCK_PRICE_FILE, 'r', encoding='utf-8') as f:
            stock_prices = json.load(f)
        logger.info(f"æˆåŠŸè®€å–è‚¡åƒ¹æ•¸æ“šï¼Œå…± {len(stock_prices)} æ”¯è‚¡ç¥¨")

        # ç²å–æ‰€æœ‰è‚¡ç¥¨ä»£ç¢¼
        stock_list = list(stock_prices.keys())
        logger.info(f"é–‹å§‹è™•ç†è‚¡ç¥¨ï¼Œç¸½å…± {len(stock_list)} æ”¯è‚¡ç¥¨")

        # è™•ç†æ¯æ”¯è‚¡ç¥¨
        all_results = []
        total_stocks = len(stock_list)
        processed_count = 0
        filtered_count = 0
        no_quarter_data_count = 0
        
        for stock_id in stock_list:
            if should_cancel:
                logger.info("æ”¶åˆ°å–æ¶ˆæŒ‡ä»¤ï¼Œåœæ­¢è™•ç†")
                await update.message.reply_text("ä»»å‹™å·²å–æ¶ˆ")
                break
                
            try:
                # ä½¿ç”¨ calculate_quarterly_stock_estimates ç²å–å­£åº¦æ•¸æ“š
                df_quarterly, now_price = await calculate_quarterly_stock_estimates(stock_id)
                if df_quarterly is None or df_quarterly.empty:
                    no_quarter_data_count += 1
                    continue

                    # è¨ˆç®—çµ±è¨ˆæ•¸æ“šçš„å¹³å‡å€¼
                avg_roe = df_quarterly['ROE'].mean()
                avg_per_high = df_quarterly['PER_æœ€é«˜å€¼'].mean()
                avg_per_normal = df_quarterly['PER_å¹³å‡å€¼'].mean()
                avg_per_low = df_quarterly['PER_æœ€ä½å€¼'].mean()

                # ä½¿ç”¨æœ€æ–°ä¸€ç­†çš„ BVPS å’Œå¹³å‡ ROE è¨ˆç®—æ¨ä¼° EPS
                latest_bvps = df_quarterly.iloc[0]['BVPS']  # æœ€æ–°ä¸€ç­†çš„ BVPS
                estimated_eps = latest_bvps * (avg_roe / 100)  # ä½¿ç”¨å¹³å‡ ROE è¨ˆç®—

                # ä½¿ç”¨æ¨ä¼° EPS å’Œå¹³å‡ PER è¨ˆç®—è‚¡åƒ¹å€é–“
                low_price = estimated_eps * avg_per_low
                normal_price = estimated_eps * avg_per_normal
                high_price = estimated_eps * avg_per_high

                # å–å¾—è©²è‚¡ç¥¨æœ€æ–°å­£åº¦æ•¸æ“š
                if avg_roe < 15:
                    logger.info(f"è‚¡ç¥¨ {stock_id} çš„ ROE ç‚º {avg_roe}ï¼Œä¸ç¬¦åˆ ROE 15% ä»¥ä¸‹çš„æ¢ä»¶")
                    continue


                # ç²å–ç•¶å‰è‚¡åƒ¹
                current_price = stock_prices.get(stock_id)
                if current_price is None or current_price <= 0:
                    continue

                # è¨ˆç®—åƒ¹å€¼åˆ†æ•¸
                # 1. è¨ˆç®—è‚¡åƒ¹ç›¸å°ä½åƒ¹çš„æŠ˜æ‰£ç¨‹åº¦ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
                price_discount = (low_price - current_price) / low_price
                
                # 2. è¨ˆç®— PER çš„æŠ˜æ‰£ç¨‹åº¦ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
                per_discount = 1 / df_quarterly.iloc[0]['PER']
                
                # 3. ç¶œåˆè¨ˆç®—åƒ¹å€¼åˆ†æ•¸ï¼ˆè€ƒæ…®è‚¡åƒ¹æŠ˜æ‰£å’Œ PER æŠ˜æ‰£ï¼‰
                value_score = (price_discount * 0.6 + per_discount * 0.4) * 100
                
                # ç¢ºä¿åˆ†æ•¸åœ¨åˆç†ç¯„åœå…§
                value_score = max(0, min(100, value_score))

                # value_score <= 0 ä¸æ¨è–¦
                if value_score <= 0:
                    logger.info(f"è‚¡ç¥¨ {stock_id} çš„åƒ¹å€¼åˆ†æ•¸ç‚º {value_score}ï¼Œä¸ç¬¦åˆåƒ¹å€¼åˆ†æ•¸ 30 ä»¥ä¸Šçš„æ¢ä»¶")
                    continue
                
                result = {
                    "stock_id": stock_id,
                    "current_price": current_price,
                    "value_score": value_score,
                    "roe": df_quarterly.iloc[0]['ROE'],
                    "price_discount": price_discount * 100,  # è½‰æ›ç‚ºç™¾åˆ†æ¯”
                    "current_per": df_quarterly.iloc[0]['PER'],
                    "roe_trend": True,  # calculate_quarterly_stock_estimates å·²ç¶“ç¢ºä¿äº† ROE è¶¨å‹¢
                    "roe_volatility": 0,  # é€™è£¡å¯ä»¥æ ¹æ“šéœ€è¦è¨ˆç®—æ³¢å‹•ç‡
                    "ä½è‚¡åƒ¹": low_price,
                    "æ­£å¸¸è‚¡åƒ¹": normal_price,
                    "é«˜è‚¡åƒ¹": high_price,
                    "æ¨ä¼°EPS": estimated_eps
                }
                
                all_results.append(result)
                filtered_count += 1

                processed_count += 1
                # æ¯è™•ç† 50 æ”¯è‚¡ç¥¨ç™¼é€ä¸€æ¬¡é€²åº¦æ›´æ–°
                if processed_count % 100 == 0:
                    progress = (processed_count / total_stocks) * 100
                    logger.info(f"è™•ç†é€²åº¦ï¼š{progress:.1f}%ï¼Œå·²è™•ç† {processed_count} æ”¯è‚¡ç¥¨ï¼Œç¬¦åˆæ¢ä»¶ {filtered_count} æ”¯ï¼Œç„¡å››å­£è³‡æ–™ {no_quarter_data_count} æ”¯")
                    await update.message.reply_text(f"è™•ç†é€²åº¦ï¼š{progress:.1f}% ({processed_count}/{total_stocks})\nç¬¦åˆæ¢ä»¶ï¼š{filtered_count} æ”¯\nç„¡å››å­£è³‡æ–™ï¼š{no_quarter_data_count} æ”¯")
                
            except Exception as e:
                logger.error(f"è™•ç†è‚¡ç¥¨ {stock_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
                continue
        
        if not all_results:
            logger.warning("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨")
            await update.message.reply_text("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨")
            return

        logger.info(f"ç¯©é¸å®Œæˆï¼Œå…±æœ‰ {len(all_results)} æ”¯è‚¡ç¥¨ç¬¦åˆæ¢ä»¶ï¼Œ{no_quarter_data_count} æ”¯è‚¡ç¥¨ç„¡å››å­£è³‡æ–™")

        # æ ¹æ“šåƒ¹å€¼åˆ†æ•¸æ’åº
        all_results.sort(key=lambda x: x["value_score"], reverse=True)
        
        # é¸å–å‰ 10 æ”¯è‚¡ç¥¨
        top_10 = all_results[:10]
        logger.info("å‰ 10 åè‚¡ç¥¨ï¼ˆä¾åƒ¹å€¼åˆ†æ•¸æ’åºï¼‰ï¼š" + ", ".join([f"{stock['stock_id']}({stock['value_score']:.2f})" for stock in top_10]))
        
        # ç”Ÿæˆæ¨è–¦è¨Šæ¯
        message = "ğŸ“Š è‚¡ç¥¨æ¨è–¦ (v2)\n\n"
        message += f"ğŸ”¹ è™•ç†çµ±è¨ˆï¼š\n"
        message += f"- ç¸½è‚¡ç¥¨æ•¸ï¼š{total_stocks} æ”¯\n"
        message += f"- ç„¡å››å­£è³‡æ–™ï¼š{no_quarter_data_count} æ”¯\n"
        message += f"- ç¬¦åˆæ¢ä»¶ï¼š{filtered_count} æ”¯\n\n"
        message += "ğŸ”¹ æ ¹æ“šåƒ¹å€¼åˆ†æ•¸æ’åºï¼š\n"
        for i, stock in enumerate(top_10, 1):
            message += f"{i}. {stock['stock_id']}\n"
            message += f"   ç•¶å‰è‚¡åƒ¹: {stock['current_price']:.2f}\n"
            message += f"   åƒ¹å€¼åˆ†æ•¸: {stock['value_score']:.2f}\n"
            message += f"   ROE: {stock['roe']:.2f}%\n"
            message += f"   æ¨ä¼°EPS: {stock['æ¨ä¼°EPS']:.2f}\n"
            message += f"   ä½è‚¡åƒ¹: {stock['ä½è‚¡åƒ¹']:.2f}\n"
            message += f"   æ­£å¸¸è‚¡åƒ¹: {stock['æ­£å¸¸è‚¡åƒ¹']:.2f}\n"
            message += f"   é«˜è‚¡åƒ¹: {stock['é«˜è‚¡åƒ¹']:.2f}\n"
            message += f"   æœ¬ç›Šæ¯”: {stock['current_per']:.2f}\n"
            message += f"   ROEè¶¨å‹¢: {'ä¸Šå‡' if stock['roe_trend'] else 'ä¸‹é™'}\n"
            message += f"   ROEæ³¢å‹•ç‡: {stock['roe_volatility']:.2f}%\n\n"
        
        logger.info("å®Œæˆæ¨è–¦è¨Šæ¯ç”Ÿæˆï¼Œæº–å‚™ç™¼é€")
        await update.message.reply_text(message)
        logger.info("æ¨è–¦è¨Šæ¯å·²ç™¼é€")
        
    except Exception as e:
        logger.error(f"æ¨è–¦è‚¡ç¥¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
        await update.message.reply_text("è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦")
    finally:
        is_processing = False
        logger.info("æ¨è–¦ä»»å‹™çµæŸ")

# æ·»åŠ æ–°çš„å¸¸é‡
ROE_DATA_FILE = "stock_roe_data.json"

async def get_stock_roe_data(update: Update, context: CallbackContext) -> None:
    """è·å–æ‰€æœ‰å°è‚¡çš„ ROE æ•°æ®å¹¶ä¿å­˜ç‚º CSV"""
    try:
        # è·å–æ‰€æœ‰å°è‚¡ä»£ç 
        stock_list = get_taiwan_stock_list()
        if not stock_list:
            await update.message.reply_text("ç„¡æ³•ç²å–å°è‚¡åˆ—è¡¨ï¼Œè«‹ç¨å¾Œå†è©¦")
            return
            
        logger.info(f"æˆåŠŸç²å– {len(stock_list)} æ”¯å°è‚¡ä»£ç¢¼")

        # åˆ›å»º CSV æ–‡ä»¶
        csv_file = "stock_roe_data.csv"
        no_data_file = "no_data_stocks.json"  # è®°å½•æ²¡æœ‰æ•°æ®çš„è‚¡ç¥¨
        processed_count = 0
        existing_stocks = set()
        no_data_stocks = set()

        # è¯»å–æ²¡æœ‰æ•°æ®çš„è‚¡ç¥¨åˆ—è¡¨
        if os.path.exists(no_data_file):
            try:
                with open(no_data_file, 'r', encoding='utf-8') as f:
                    no_data_stocks = set(json.load(f))
                    logger.info(f"è®€å–åˆ° {len(no_data_stocks)} æ”¯æ²’æœ‰æ•¸æ“šçš„è‚¡ç¥¨")
            except Exception as e:
                logger.error(f"è®€å–ç„¡æ•¸æ“šè‚¡ç¥¨åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                no_data_stocks = set()

        # æ£€æŸ¥ç°æœ‰çš„ CSV æ–‡ä»¶
        if os.path.exists(csv_file):
            try:
                df_existing = pd.read_csv(csv_file)
                # ç¡®ä¿ stock_id ä¸ºå­—ç¬¦ä¸²ç±»å‹
                df_existing['stock_id'] = df_existing['stock_id'].astype(str)
                existing_stocks = set(df_existing['stock_id'].unique())
                logger.info(f"ç¾æœ‰ CSV æ–‡ä»¶ä¸­å·²æœ‰ {len(existing_stocks)} æ”¯è‚¡ç¥¨çš„æ•¸æ“š")
                logger.info(f"CSV ä¸­çš„è‚¡ç¥¨ç¤ºä¾‹ï¼š{list(existing_stocks)[:5]}")
            except Exception as e:
                logger.error(f"è®€å–ç¾æœ‰ CSV æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                existing_stocks = set()

        logger.info(f"æœ¬æ¬¡å°‡è™•ç†å‰ {len(stock_list)} æ”¯è‚¡ç¥¨")

        # è·å–éœ€è¦å¤„ç†çš„è‚¡ç¥¨åˆ—è¡¨ï¼ˆæ’é™¤å·²æœ‰æ•°æ®çš„è‚¡ç¥¨ï¼‰
        missing_stocks = [stock_id for stock_id in stock_list 
                        if stock_id not in existing_stocks and stock_id not in no_data_stocks]
        logger.info(f"å…¶ä¸­ {len(missing_stocks)} æ”¯è‚¡ç¥¨éœ€è¦è™•ç†")

        if not missing_stocks:
            await update.message.reply_text("æ‰€æœ‰è‚¡ç¥¨æ•¸æ“šéƒ½å·²æ˜¯æœ€æ–°")
            return

        # å¤„ç†ç¼ºå¤±çš„è‚¡ç¥¨
        new_no_data_stocks = set()  # è®°å½•æœ¬æ¬¡æ‰§è¡Œä¸­å‘ç°æ²¡æœ‰æ•°æ®çš„è‚¡ç¥¨
        for stock_id in missing_stocks:
            try:
                logger.info(f"æ­£åœ¨æŸ¥è©¢è‚¡ç¥¨ {stock_id} çš„ ROE æ•¸æ“š...")
                # è®¾ç½® API å‚æ•°
                parameter = {
                    "dataset": "TaiwanStockPER",
                    "data_id": stock_id,
                    "start_date": "2020-01-01",
                    "end_date": datetime.now().strftime('%Y-%m-%d'),
                    "token": FINMIND_API_KEY,
                }

                # è·å– ROE æ•°æ®
                async with aiohttp.ClientSession() as session:
                    async with session.get(FINMIND_URL, params=parameter) as response:
                        if response.status != 200:
                            logger.error(f"API è«‹æ±‚å¤±æ•—ï¼Œè‚¡ç¥¨ {stock_id}ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status}")
                            new_no_data_stocks.add(stock_id)
                            continue

                        data = await response.json()
                        if "data" not in data or not isinstance(data["data"], list) or len(data["data"]) == 0:
                            logger.warning(f"è‚¡ç¥¨ {stock_id} æ²’æœ‰æ•¸æ“š")
                            new_no_data_stocks.add(stock_id)
                            continue

                        logger.info(f"æˆåŠŸç²å–è‚¡ç¥¨ {stock_id} çš„ ROE æ•¸æ“š")

                        # è½¬æ¢æ•°æ®ä¸º DataFrame
                        df = pd.DataFrame(data["data"])
                        
                        # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
                        df["date"] = pd.to_datetime(df["date"])
                        
                        # ç¡®ä¿æ•°å€¼å­—æ®µä¸ºæ•°å€¼ç±»å‹
                        numeric_columns = ["PER", "PBR", "ROE"]
                        for col in numeric_columns:
                            if col in df.columns:
                                df[col] = pd.to_numeric(df[col], errors="coerce")

                        # æ·»åŠ è‚¡ç¥¨ä»£ç åˆ—
                        df["stock_id"] = stock_id

                        # å°†æ•°æ®å†™å…¥ CSV
                        if processed_count == 0 and not os.path.exists(csv_file):
                            # ç¬¬ä¸€æ¬¡å†™å…¥ï¼Œåˆ›å»ºæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
                            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                        else:
                            # åç»­å†™å…¥ï¼Œè¿½åŠ æ•°æ®ï¼ˆä¸åŒ…å«è¡¨å¤´ï¼‰
                            df.to_csv(csv_file, mode='a', header=False, index=False, encoding='utf-8-sig')

                        processed_count += 1

                        # æ¯å¤„ç† 10 æ”¯è‚¡ç¥¨å‘é€ä¸€æ¬¡è¿›åº¦æ›´æ–°
                        if processed_count % 10 == 0:
                            logger.info(f"å·²è™•ç† {processed_count}/{len(missing_stocks)} æ”¯è‚¡ç¥¨")
                            await update.message.reply_text(f"å·²è™•ç† {processed_count}/{len(missing_stocks)} æ”¯è‚¡ç¥¨")

                        # ç­‰å¾… 0.5 ç§’å†å¤„ç†ä¸‹ä¸€æ”¯è‚¡ç¥¨
                        await asyncio.sleep(0.5)

            except Exception as e:
                logger.error(f"è™•ç†è‚¡ç¥¨ {stock_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                new_no_data_stocks.add(stock_id)
                continue

        # æ›´æ–°æ²¡æœ‰æ•°æ®çš„è‚¡ç¥¨åˆ—è¡¨
        if new_no_data_stocks:
            no_data_stocks.update(new_no_data_stocks)
            with open(no_data_file, 'w', encoding='utf-8') as f:
                json.dump(list(no_data_stocks), f, ensure_ascii=False, indent=2)
            logger.info(f"æ–°å¢ {len(new_no_data_stocks)} æ”¯æ²’æœ‰æ•¸æ“šçš„è‚¡ç¥¨åˆ°è¨˜éŒ„ä¸­")

        await update.message.reply_text(f"å®Œæˆï¼å…±è™•ç† {processed_count} æ”¯è‚¡ç¥¨æ•¸æ“š")

    except Exception as e:
        logger.error(f"ç²å–è‚¡ç¥¨ ROE æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        await update.message.reply_text("è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦")

# æ·»åŠ æ–°çš„å¸¸é‡
STOCK_PRICE_FILE = "stock_prices.json"

async def sync_stock_prices(update: Update, context: CallbackContext) -> None:
    """åŒæ­¥æ‰€æœ‰è‚¡ç¥¨çš„æœ€æ–°åƒ¹æ ¼ä¸¦ä¿å­˜åˆ° JSON æ–‡ä»¶"""
    try:
        # ä½¿ç”¨ get_taiwan_stock_list() ç²å–è‚¡ç¥¨åˆ—è¡¨
        stock_list = get_taiwan_stock_list()
        if not stock_list:
            await update.message.reply_text("ç„¡æ³•ç²å–è‚¡ç¥¨åˆ—è¡¨ï¼Œè«‹ç¨å¾Œå†è©¦")
            return

        logger.info(f"éœ€è¦æ›´æ–° {len(stock_list)} æ”¯è‚¡ç¥¨çš„åƒ¹æ ¼")

        # ç™¼é€é–‹å§‹æ›´æ–°çš„è¨Šæ¯
        status_message = await update.message.reply_text("é–‹å§‹æ›´æ–°è‚¡ç¥¨åƒ¹æ ¼...")
        processed_count = 0
        updated_prices = {}

        # è™•ç†æ¯æ”¯è‚¡ç¥¨
        for stock_id in stock_list:
            try:
                # ä½¿ç”¨ get_current_stock_price ç²å–åƒ¹æ ¼
                current_price = await get_current_stock_price(stock_id)
                if current_price is not None:
                    updated_prices[stock_id] = current_price
                    processed_count += 1

                # æ¯è™•ç† 100 æ”¯è‚¡ç¥¨æ›´æ–°ä¸€æ¬¡é€²åº¦
                if processed_count % 100 == 0:
                    progress = (processed_count / len(stock_list)) * 100
                    await status_message.edit_text(f"æ›´æ–°é€²åº¦ï¼š{progress:.1f}% ({processed_count}/{len(stock_list)})")

                # æ¯æ”¯è‚¡ç¥¨è™•ç†å®Œå¾Œæš«åœä¸€ä¸‹
                await asyncio.sleep(0.1)

            except Exception as e:
                logger.error(f"è™•ç†è‚¡ç¥¨ {stock_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue

        # ä¿å­˜æ›´æ–°å¾Œçš„åƒ¹æ ¼æ•¸æ“š
        with open(STOCK_PRICE_FILE, 'w', encoding='utf-8') as f:
            json.dump(updated_prices, f, ensure_ascii=False, indent=2)

        # ç™¼é€å®Œæˆè¨Šæ¯
        await update.message.reply_text(f"è‚¡ç¥¨åƒ¹æ ¼æ›´æ–°å®Œæˆï¼å…±æ›´æ–° {len(updated_prices)} æ”¯è‚¡ç¥¨çš„åƒ¹æ ¼")

    except Exception as e:
        logger.error(f"åŒæ­¥è‚¡ç¥¨åƒ¹æ ¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        await update.message.reply_text("è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦")

async def update_csv_with_close(update: Update, context: CallbackContext) -> None:
    """
    è®€å– stock_roe_data.csvï¼Œå°æ–¼æ¯ä¸€å€‹å”¯ä¸€çš„ (stock_id, date) çµ„åˆï¼Œ
    å‘¼å« API ç²å–è©²æ—¥çš„æ”¶ç›¤åƒ¹ (close)ï¼Œ
    ä¸¦å°‡çµæœåˆä½µé€² CSVï¼ˆä¾æ“š stock_id èˆ‡ date åŒ¹é…ï¼‰ï¼Œæ›´æ–°å¾Œå­˜å› CSV æ–‡ä»¶ã€‚
    åŒæ™‚æ›´æ–° stock_prices.json ä¸­çš„æœ€æ–°åƒ¹æ ¼ã€‚
    """
    csv_file = "stock_roe_data.csv"
    if not os.path.exists(csv_file):
        logger.error(f"æ‰¾ä¸åˆ° {csv_file} æ–‡ä»¶")
        return

    # è®€å– CSV
    df = pd.read_csv(csv_file)
    
    # å°‡ date æ¬„ä½è½‰æ›ç‚º datetimeï¼ˆè‹¥å°šæœªè½‰æ›ï¼‰
    df["date"] = pd.to_datetime(df["date"].str.strip(), format="%Y-%m-%d", errors="coerce")
    
    # ç¢ºä¿ stock_id ç‚ºå­—ä¸²é¡å‹
    df["stock_id"] = df["stock_id"].astype(str)
    
    # å¦‚æœæ²’æœ‰ close æ¬„ä½ï¼Œæ–°å¢ä¸€å€‹ï¼ˆå¯ä»¥å…ˆå¡«å…… NaNï¼‰
    if "close" not in df.columns:
        df["close"] = np.nan

    # ç²å–dfä¸­çš„æ‰€æœ‰stock_id
    stock_ids = df["stock_id"].unique().tolist()
    
    # å»ºç«‹ä¸€å€‹åˆ—è¡¨ä¾†å­˜æ”¾æ‰€æœ‰æ”¶ç›¤åƒ¹è³‡æ–™
    all_price_dfs = []
    
    # ç”¨æ–¼æ›´æ–° stock_prices.json çš„æœ€æ–°åƒ¹æ ¼
    latest_prices = {}

    # å–å¾—dfä¸­çš„ç¬¬ä¸€ç­†è³‡æ–™çš„date
    query_date = df["date"].iloc[0]
    
    # éæ­·æ¯å€‹è‚¡ç¥¨ä»£ç¢¼
    for stock_id in stock_ids:
        try:
            # ç²å–è©²è‚¡ç¥¨çš„æ‰€æœ‰æ—¥æœŸ
            stock_dates = df[df["stock_id"] == stock_id]["date"].unique()

            logger.info(f"æ­£åœ¨æŸ¥è©¢è‚¡ç¥¨ {stock_id} åœ¨ {query_date.strftime('%Y-%m-%d')} çš„æ”¶ç›¤åƒ¹")
                
            # å‘¼å« API ç²å–æ”¶ç›¤åƒ¹
            price_df = await get_stock_price_from_date(stock_id, query_date)
            if price_df is not None and not price_df.empty:
                # ç¢ºä¿ price_df çš„ stock_id ä¹Ÿæ˜¯å­—ä¸²é¡å‹
                price_df["stock_id"] = price_df["stock_id"].astype(str)
                all_price_dfs.append(price_df)
                
                # ç²å–æœ€æ–°åƒ¹æ ¼ä¸¦æ›´æ–°åˆ° latest_prices
                latest_price = price_df.sort_values("date").iloc[-1]["close"]
                if latest_price > 0:  # ç¢ºä¿åƒ¹æ ¼æœ‰æ•ˆ
                    latest_prices[stock_id] = latest_price
                
            # é©ç•¶å»¶é²ï¼Œé¿å… API è«‹æ±‚éå¿«
            await asyncio.sleep(0.1)
                
        except Exception as e:
            logger.error(f"è™•ç†è‚¡ç¥¨ {stock_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            continue

    if not all_price_dfs:
        logger.warning("æ²’æœ‰ç²å–åˆ°ä»»ä½•æ”¶ç›¤åƒ¹æ•¸æ“š")
        return

    # åˆä½µæ‰€æœ‰æ”¶ç›¤åƒ¹æ•¸æ“š
    df_prices = pd.concat(all_price_dfs, ignore_index=True)
    
    # ç¢ºä¿ df_prices çš„ date æ¬„ä½ä¹Ÿæ˜¯ datetime
    df_prices["date"] = pd.to_datetime(df_prices["date"].str.strip(), format="%Y-%m-%d", errors="coerce")
    
    # åˆä½µåŸ CSV èˆ‡æ”¶ç›¤åƒ¹è³‡æ–™ï¼Œæ ¹æ“š stock_id èˆ‡ date é€²è¡ŒåŒ¹é…
    df_updated = pd.merge(df, df_prices, on=["stock_id", "date"], how="left", suffixes=("", "_new"))
    
    # æ›´æ–°åŸæœ¬çš„ close æ¬„ä½ç‚ºå¾ API ç²å¾—çš„æ–°æ•¸æ“š
    if "close_new" in df_updated.columns:
        df_updated["close"] = df_updated["close_new"]
        df_updated.drop(columns=["close_new"], inplace=True)

    # ä¿å­˜æ›´æ–°å¾Œçš„ CSV æ–‡ä»¶
    df_updated.to_csv(csv_file, index=False, encoding='utf-8-sig')
    logger.info(f"å·²å°‡æ”¶ç›¤åƒ¹æ•¸æ“šæ›´æ–°ä¸¦åˆä½µåˆ° {csv_file} æ–‡ä»¶ä¸­")

    # æ›´æ–° stock_prices.json
    if latest_prices:
        with open(STOCK_PRICE_FILE, 'w', encoding='utf-8') as f:
            json.dump(latest_prices, f, ensure_ascii=False, indent=2)
        logger.info(f"å·²æ›´æ–° {len(latest_prices)} æ”¯è‚¡ç¥¨çš„æœ€æ–°åƒ¹æ ¼åˆ° stock_prices.json")

    # ç™¼é€å®Œæˆè¨Šæ¯
    await update.message.reply_text(
        f"è‚¡ç¥¨åƒ¹æ ¼æ›´æ–°å®Œæˆï¼\n"
        f"- æ›´æ–° {len(latest_prices)} æ”¯è‚¡ç¥¨çš„æœ€æ–°åƒ¹æ ¼\n"
        f"- æ›´æ–° {len(all_price_dfs)} æ”¯è‚¡ç¥¨çš„æ­·å²æ”¶ç›¤åƒ¹"
    )


def main():
    global app
    load_dotenv()
    
    BOT_TOKEN = os.getenv("BOT_TOKEN")
    if not BOT_TOKEN:
        raise ValueError("æœªæ‰¾åˆ° BOT_TOKENï¼Œè«‹åœ¨ Heroku ç’°å¢ƒè®Šæ•¸è¨­å®š BOT_TOKEN")
    
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)
    
    try:
        app = Application.builder().token(BOT_TOKEN).build()

        app.add_handler(CommandHandler("start", start))
        app.add_handler(CommandHandler("get_roe_data", get_stock_roe_data))
        app.add_handler(CommandHandler("recommend_v2", recommend_v2))
        app.add_handler(CommandHandler("cancel_recommend", cancel_recommend))
        app.add_handler(CommandHandler("etf", etf))
        app.add_handler(CommandHandler("stock_estimate", stock_estimate))
        app.add_handler(CommandHandler("update_csv_with_close", update_csv_with_close))

        logger.info("Bot å·²å•Ÿå‹•ä¸¦é–‹å§‹é‹è¡Œ...")
        app.run_polling()
        
    except Exception as e:
        logger.error(f"é‹è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        if app:
            app.stop()
        sys.exit(1)

if __name__ == "__main__":
    main()
