import pandas as pd
import logging
import os
import requests
import signal
import sys
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext
from dotenv import load_dotenv
from datetime import datetime, timedelta
import numpy as np
import asyncio
import json
import aiohttp
import time
from typing import Dict, List, Optional
import pickle

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,
    encoding='utf-8'  # æ·»åŠ  UTF-8 ç·¨ç¢¼
)
logger = logging.getLogger(__name__)

# å‰µå»ºä¸€å€‹å…¨å±€çš„ application è®Šé‡
app = None

# æ·»åŠ å…¨å±€è®Šé‡ä¾†è¿½è¹¤åŸ·è¡Œç‹€æ…‹
is_processing = False
current_task = None
should_cancel = False

# ç·©å­˜ç›¸é—œå¸¸é‡
CACHE_FILE = "stock_data_cache.pkl"
CACHE_EXPIRY_DAYS = 7  # æ”¹ç‚º 7 å¤©ï¼Œå› ç‚ºåŸºæœ¬é¢æ•¸æ“šè®ŠåŒ–è¼ƒæ…¢
BATCH_SIZE = 100  # å¢åŠ æ‰¹æ¬¡å¤§å°
DELAY_BETWEEN_BATCHES = 1  # æ¸›å°‘æ‰¹æ¬¡é–“å»¶é²åˆ° 30 ç§’
MAX_CONCURRENT_REQUESTS = 10  # å¢åŠ ä¸¦ç™¼è«‹æ±‚æ•¸

# ç·©å­˜æ•¸æ“šçµæ§‹
class StockDataCache:
    def __init__(self):
        self.data: Dict[str, Dict] = {}
        self.last_update: Dict[str, datetime] = {}
    
    def is_valid(self, stock_id: str) -> bool:
        if stock_id not in self.last_update:
            return False
        return (datetime.now() - self.last_update[stock_id]).days < CACHE_EXPIRY_DAYS
    
    def get(self, stock_id: str) -> Optional[Dict]:
        if self.is_valid(stock_id):
            return self.data.get(stock_id)
        return None
    
    def set(self, stock_id: str, data: Dict):
        self.data[stock_id] = data
        self.last_update[stock_id] = datetime.now()
    
    def save(self):
        with open(CACHE_FILE, 'wb') as f:
            pickle.dump(self, f)
    
    @classmethod
    def load(cls) -> 'StockDataCache':
        try:
            with open(CACHE_FILE, 'rb') as f:
                return pickle.load(f)
        except (FileNotFoundError, pickle.PickleError):
            return cls()

# å…¨å±€ç·©å­˜å°è±¡
stock_cache = StockDataCache.load()

# ä¿¡è™Ÿè™•ç†å‡½æ•¸
def signal_handler(signum, frame):
    logger.info("æ”¶åˆ°çµ‚æ­¢ä¿¡è™Ÿï¼Œæ­£åœ¨å„ªé›…é€€å‡º...")
    if app:
        logger.info("æ­£åœ¨åœæ­¢ Telegram Bot...")
        app.stop()
    sys.exit(0)

# print("ç•¶å‰å·¥ä½œç›®éŒ„:", os.getcwd())

import pandas as pd
from telegram import Update
from telegram.ext import CallbackContext

load_dotenv()
FINMIND_API_KEY = os.getenv("FINMIND_API_KEY")
FINMIND_URL = "https://api.finmindtrade.com/api/v4/data"

# è®€å–è‚¡ç¥¨åŸºæœ¬è³‡è¨Š CSV
CSV_FILE = "Calculated_Stock_Values.csv"
df = pd.read_csv(CSV_FILE)

# ç¢ºä¿ "ä»£è™Ÿ" æ¬„ä½ç‚ºå­—ä¸²
df["ä»£è™Ÿ"] = df["ä»£è™Ÿ"].astype(str)

# è®€å–é…æ¯è³‡è¨Š CSV
DIVIDEND_CSV_FILE = "all_stock_dividends.csv"
df_dividend = pd.read_csv(DIVIDEND_CSV_FILE)

# ç¢ºä¿ "stock_id" æ¬„ä½ç‚ºå­—ä¸²
df_dividend["stock_id"] = df_dividend["stock_id"].astype(str)

# ç¢ºä¿ "CashEarningsDistribution" æ¬„ä½æ˜¯æ•¸å€¼é¡å‹ï¼ˆé¿å… NaN å•é¡Œï¼‰
df_dividend["CashEarningsDistribution"] = pd.to_numeric(df_dividend["CashEarningsDistribution"], errors='coerce')

# è¨­å®šæ©Ÿå™¨äºº
async def start(update: Update, context: CallbackContext) -> None:
    await update.message.reply_text("æ­¡è¿ä½¿ç”¨è‚¡ç¥¨æŸ¥è©¢æ©Ÿå™¨äººï¼è«‹è¼¸å…¥ /stock <è‚¡ç¥¨ä»£è™Ÿ> æˆ– /recommend")


# Telegram Bot æŒ‡ä»¤ï¼š/stock_estimate 2330
async def stock_estimate(update: Update, context: CallbackContext) -> None:
    if not context.args:
        await update.message.reply_text("è«‹è¼¸å…¥è‚¡ç¥¨ä»£è™Ÿï¼Œä¾‹å¦‚ï¼š/stock_estimate 2330")
        return

    stock_id = context.args[0]
    df_result = await calculate_quarterly_stock_estimates(stock_id)

    if df_result is None:
        await update.message.reply_text(f"âš ï¸ ç„¡æ³•ç²å– {stock_id} çš„æ•¸æ“šï¼Œè«‹æª¢æŸ¥ API è¨­å®šæˆ–è‚¡ç¥¨ä»£è™Ÿ")
        return

    # å–æœ€è¿‘ 4 å­£æ•¸æ“š
    df_result = df_result.tail(4)

    # ç”Ÿæˆå›æ‡‰è¨Šæ¯
    message = f"ğŸ“Š **{stock_id} å­£åº¦ ROE & æ¨ä¼°è‚¡åƒ¹** ğŸ“Š\n"
    for _, row in df_result.iterrows():
        message += (
            f"\nğŸ“… **å­£åº¦**: {row['quarter']}"
            f"\nğŸ“Š **ROE**: {row['ROE']:.2f}%"
            f"\nğŸ¦ **BVPS**: {row['BVPS']:.2f} å…ƒ"
            f"\nğŸ’° **æ¨ä¼°EPS**: {row['æ¨ä¼°EPS']:.2f} å…ƒ"
            f"\nğŸ“ˆ **PER å€é–“**: {row['PER_æœ€ä½å€¼']:.2f} ~ {row['PER_æœ€é«˜å€¼']:.2f}"
            f"\nğŸ“‰ **ä½è‚¡åƒ¹**: {row['ä½è‚¡åƒ¹']:.2f} å…ƒ"
            f"\nğŸ“Š **æ­£å¸¸è‚¡åƒ¹**: {row['æ­£å¸¸è‚¡åƒ¹']:.2f} å…ƒ"
            f"\nğŸ“ˆ **é«˜è‚¡åƒ¹**: {row['é«˜è‚¡åƒ¹']:.2f} å…ƒ"
            f"\n--------------------"
        )

    await update.message.reply_text(message, parse_mode="Markdown")


async def etf(update: Update, context: CallbackContext) -> None:
    if not context.args:
        await update.message.reply_text("è«‹è¼¸å…¥ ETF ä»£è™Ÿï¼Œä¾‹å¦‚ï¼š/etf 00713")
        return
    
    # ğŸ”¹ æŸ¥è©¢ç•¶å‰è‚¡åƒ¹
    stock_id = context.args[0]
    current_price = await get_current_stock_price(stock_id)

    if current_price is None:
        await update.message.reply_text(f"ç„¡æ³•ç²å– {stock_id} çš„æœ€æ–°è‚¡åƒ¹ï¼Œè«‹ç¨å¾Œå†è©¦")
        return

    # ğŸ”¹ è¨ˆç®—æœ€è¿‘ä¸€å¹´é…æ¯ç¸½é¡ & æ®–åˆ©ç‡
    # total_dividends, dividend_yield = calculate_dividend_yield(stock_id, current_price)
    total_dividends, dividend_yield, dividends_count = calculate_all_dividend_yield(stock_id, current_price)

    # ğŸ”¹ å›æ‡‰è¨Šæ¯
    message = (
        f"ğŸ“Š **ETF è³‡è¨Š - {stock_id}**\n"
        f"ğŸ”¹ **ç•¶å‰è‚¡åƒ¹**: {current_price:.2f} å…ƒ\n"
        f"ğŸ’¸ **æœ€è¿‘ä¸€å¹´é…æ¯ç¸½é¡**: {total_dividends:.2f} å…ƒ ğŸ’°\n"
        f"ğŸ“Š **æ®–åˆ©ç‡**: {dividend_yield:.2f}%\n"
        f"ğŸ”¹ **é…æ¯ç­†æ•¸**: {dividends_count} ç­†\n"
    )
    
    await update.message.reply_text(message, parse_mode="Markdown")


# ä¿®æ”¹ get_current_stock_price å‡½æ•¸ç‚ºç•°æ­¥å‡½æ•¸
async def get_current_stock_price(stock_id):
    """è·å–è‚¡ç¥¨å½“å‰ä»·æ ¼ï¼Œç›´æ¥ä» API è·å–æœ€è¿‘5å¤©çš„æ•°æ®"""
    try:
        # è·å–æœ€è¿‘5å¤©çš„æ•°æ®
        parameter = {
            "dataset": "TaiwanStockPrice",
            "start_date": (datetime.today() - timedelta(days=5)).strftime('%Y-%m-%d'),
            "token": FINMIND_API_KEY,
        }

        async with aiohttp.ClientSession() as session:
            async with session.get(FINMIND_URL, params=parameter) as response:
                if response.status != 200:
                    logger.error(f"API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status}")
                    return None

                data = await response.json()

                # æ£€æŸ¥ API å›åº”æ˜¯å¦æœ‰æ•°æ®
                if "data" in data and isinstance(data["data"], list) and len(data["data"]) > 0:
                    df_price = pd.DataFrame(data["data"])
                    
                    # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
                    df_price['date'] = pd.to_datetime(df_price['date'])
                    
                    # ç¡®ä¿æ•°å€¼å­—æ®µä¸ºæ•°å€¼ç±»å‹
                    numeric_columns = ['Trading_Volume', 'Trading_money', 'open', 'max', 'min', 'close', 'spread', 'Trading_turnover']
                    for col in numeric_columns:
                        if col in df_price.columns:
                            df_price[col] = pd.to_numeric(df_price[col], errors='coerce')
                    
                    # è·å–æŒ‡å®šè‚¡ç¥¨çš„æœ€æ–°æ”¶ç›˜ä»·
                    stock_data = df_price[df_price['stock_id'] == stock_id]
                    if not stock_data.empty:
                        latest_price = stock_data.sort_values('date').iloc[-1]['close']
                        logger.info(f"æˆåŠŸè·å–è‚¡ç¥¨ {stock_id} çš„æœ€æ–°ä»·æ ¼ï¼š{latest_price}")
                        return latest_price
                    else:
                        logger.warning(f"æœªæ‰¾åˆ°è‚¡ç¥¨ {stock_id} çš„ä»·æ ¼æ•°æ®")
                else:
                    logger.warning("API è¿”å›æ•°æ®ä¸ºç©º")

        return None

    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨ {stock_id} ä»·æ ¼æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return None
    

def calculate_dividend_yield(stock_id, current_price):
    """ è¨ˆç®—è©² ETF æˆ–è‚¡ç¥¨çš„æœ€è¿‘ä¸€å¹´åº¦é…æ¯ç¸½é¡ï¼Œä¸¦è¨ˆç®—æ®–åˆ©ç‡ """
    
    # éæ¿¾ç‰¹å®šè‚¡ç¥¨
    stock_dividends = df_dividend[(df_dividend["stock_id"] == stock_id) & (df_dividend["CashEarningsDistribution"] > 0)].copy()

    # ç¢ºä¿ date æ¬„ä½æ˜¯ datetime æ ¼å¼
    stock_dividends["date"] = pd.to_datetime(stock_dividends["date"], errors="coerce")

    if stock_dividends.empty:
        return 0.0, 0.0  # å¦‚æœè©²è‚¡ç¥¨ç„¡é…æ¯è³‡æ–™ï¼Œå‰‡å›å‚³ 0

    # ğŸ”¹ å–å¾—æœ€è¿‘ä¸€å¹´çš„é…æ¯
    one_year_ago = datetime.today() - timedelta(days=365)
    
    # **é€™è¡ŒéŒ¯èª¤çš„æ¯”è¼ƒæ”¹ç‚ºç¢ºä¿ date æ¬„ä½æ˜¯ datetime**
    last_year_dividends = stock_dividends[stock_dividends["date"] >= one_year_ago]

    # è¨ˆç®—å¹´åº¦é…æ¯ç¸½é¡
    total_dividends = last_year_dividends["CashEarningsDistribution"].sum()

    # è¨ˆç®—æ®–åˆ©ç‡
    if current_price > 0:
        dividend_yield = (total_dividends / current_price) * 100
    else:
        dividend_yield = 0.0

    return total_dividends, dividend_yield


# ğŸ”¹ æŸ¥è©¢é…æ¯è³‡æ–™ä¸¦è¨ˆç®—å®Œæ•´æ®–åˆ©ç‡
def calculate_all_dividend_yield(stock_id, current_price):
    """ 
    è¨ˆç®—å®Œæ•´æ®–åˆ©ç‡ï¼ˆåŒ…å«ç¾é‡‘èˆ‡è‚¡ç¥¨è‚¡åˆ©ï¼‰ 
    """
    # ğŸ”¹ éæ¿¾è©²è‚¡ç¥¨çš„é…æ¯è³‡æ–™
    stock_dividends = df_dividend[df_dividend["stock_id"] == stock_id].copy()

    # ç¢ºä¿ date æ¬„ä½æ˜¯ datetime æ ¼å¼
    stock_dividends["date"] = pd.to_datetime(stock_dividends["date"], errors="coerce")
    
    if stock_dividends.empty:
        return 0.0, 0.0, 0  # å¦‚æœè©²è‚¡ç¥¨ç„¡é…æ¯è³‡æ–™ï¼Œå‰‡å›å‚³ 0

    # å…ˆæŒ‰ç…§æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    stock_dividends = stock_dividends.sort_values(by="date", ascending=False)

    # å–å¾—æœ€è¿‘ä¸€å¹´çš„é…æ¯
    one_year_ago = datetime.today() - timedelta(days=365)
    today = datetime.today()
    
    # å–å¾—æœ€è¿‘ä¸€å¹´çš„é…æ¯è³‡æ–™ï¼Œä¸¦ç¢ºä¿ä¸é‡è¤‡
    last_year_dividends = stock_dividends[
        (stock_dividends["date"] >= one_year_ago) & 
        (stock_dividends["date"] <= today) &  # æ’é™¤æœªä¾†çš„é…æ¯æ—¥æœŸ
        (stock_dividends["CashEarningsDistribution"] > 0)  # åªå–æœ‰ç¾é‡‘è‚¡åˆ©çš„è³‡æ–™
    ].drop_duplicates(subset=["date"])  # ç§»é™¤åŒä¸€å¤©çš„é‡è¤‡è³‡æ–™
    
    # ç¢ºä¿è‡³å°‘æœ‰ 1 ç­†é…æ¯è³‡æ–™
    if last_year_dividends.empty:
        return 0.0, 0.0, 0

    # è¨ˆç®—æœ€è¿‘ä¸€å¹´çš„ **ç¾é‡‘è‚¡åˆ©ç¸½é¡**
    total_cash_dividends = last_year_dividends["CashEarningsDistribution"].sum()

    # è¨ˆç®—æœ€è¿‘ä¸€å¹´çš„ **è‚¡ç¥¨è‚¡åˆ©ç¸½é¡**
    total_stock_dividends = last_year_dividends["StockEarningsDistribution"].sum()

    # **è¨ˆç®—é™¤æ¬Šæ¯å¾Œè‚¡åƒ¹**
    ex_rights_price = max(current_price - total_cash_dividends, 0)  # ç¢ºä¿è‚¡åƒ¹ä¸ç‚ºè² 

    # **è¨ˆç®—è‚¡ç¥¨è‚¡åˆ©åƒ¹å€¼**
    stock_dividend_value = total_stock_dividends * ex_rights_price / 1000

    # **è¨ˆç®—ç¸½è‚¡åˆ©åƒ¹å€¼**
    total_dividend_value = stock_dividend_value + (total_cash_dividends)

    # **è¨ˆç®—é‚„åŸæ®–åˆ©ç‡**
    if current_price > 0:
        restored_dividend_yield = (total_dividend_value / current_price) * 100.00
    else:
        restored_dividend_yield = 0.0

    return total_dividend_value, restored_dividend_yield, len(last_year_dividends)


# ä¿®æ”¹ calculate_quarterly_stock_estimates å‡½æ•¸ç‚ºç•°æ­¥å‡½æ•¸
async def calculate_quarterly_stock_estimates(stock_id, start_date="2020-01-01", end_date=None):
    """ å¾ CSV æ–‡ä»¶è®€å–æ•¸æ“šï¼Œè¨ˆç®—å­£åº¦ ROEã€BVPSã€æ¨ä¼°è‚¡åƒ¹ """
    try:
        # è®€å– CSV æ–‡ä»¶
        csv_file = "stock_roe_data.csv"
        if not os.path.exists(csv_file):
            logger.error(f"æ‰¾ä¸åˆ° {csv_file} æ–‡ä»¶")
            return None

        # è®€å–æ•¸æ“šä¸¦éæ¿¾æŒ‡å®šè‚¡ç¥¨
        df = pd.read_csv(csv_file)
        df['stock_id'] = df['stock_id'].astype(str)
        df = df[df['stock_id'] == stock_id]

        if df.empty:
            logger.warning(f"è‚¡ç¥¨ {stock_id} åœ¨ CSV ä¸­æ²’æœ‰æ•¸æ“š")
            return None

        # ç¢ºä¿æ—¥æœŸæ ¼å¼æ­£ç¢º
        df["date"] = pd.to_datetime(df["date"])
        
        # ç¢ºä¿æ•¸å€¼æ¬„ä½ç‚ºæ•¸å€¼é¡å‹
        numeric_columns = ["PER", "PBR"]
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")

        # éæ¿¾ç„¡æ•ˆçš„ PER å’Œ PBR æ•¸æ“š
        df = df[
            (df['PER'] > 0) & (df['PER'] < 100) &  # åˆç†çš„ PER ç¯„åœ
            (df['PBR'] > 0) & (df['PBR'] < 10)     # åˆç†çš„ PBR ç¯„åœ
        ]

        if df.empty:
            logger.warning(f"è‚¡ç¥¨ {stock_id} æ²’æœ‰æœ‰æ•ˆçš„ PER å’Œ PBR æ•¸æ“š")
            return None

        # è¨ˆç®— ROE (%)
        df["ROE"] = np.where(
            (df['PER'] != 0) & (df['PER'].notna()) & (df['PBR'].notna()),
            (df['PBR'] / df['PER']) * 100,
            np.nan
        )

        # ä¾å­£åº¦å–æ•¸æ“š
        df["quarter"] = df["date"].dt.to_period("Q")
        
        # è¨ˆç®—å­£åº¦ PER çµ±è¨ˆæ•¸æ“šï¼ˆä½¿ç”¨ç™¾åˆ†ä½æ•¸é¿å…æ¥µç«¯å€¼ï¼‰
        df_per_stats = df.groupby("quarter").agg(
            PER_æœ€é«˜å€¼=("PER", lambda x: np.percentile(x, 95)),  # 95th percentile
            PER_å¹³å‡å€¼=("PER", "mean"),
            PER_æœ€ä½å€¼=("PER", lambda x: np.percentile(x, 5))    # 5th percentile
        ).reset_index()

        # è¨ˆç®—æ¯å­£åº¦çš„å¹³å‡å€¼
        df_quarterly = df.groupby("quarter").agg(
            date=("date", "last"),
            PER=("PER", "mean"),
            PBR=("PBR", "median"),
            ROE=("ROE", "median")
        ).reset_index()

        # åˆä½µ PER çµ±è¨ˆæ•¸æ“š
        df_quarterly = df_quarterly.merge(df_per_stats, on="quarter", how="left")

        # å–å¾—ç›®å‰è‚¡åƒ¹
        current_price = await get_current_stock_price(stock_id)
        if current_price is None or current_price <= 0:
            logger.warning(f"è‚¡ç¥¨ {stock_id} ç„¡æ³•ç²å–æœ‰æ•ˆçš„ç•¶å‰è‚¡åƒ¹")
            return None

        # è¨ˆç®— BVPS
        df_quarterly["prev_close"] = current_price
        df_quarterly["BVPS"] = df_quarterly["prev_close"] / df_quarterly["PBR"]

        # è¨ˆç®—æ¨ä¼°EPS
        df_quarterly["æ¨ä¼°EPS"] = df_quarterly["BVPS"] * (df_quarterly["ROE"] / 100)

        # è¨ˆç®—ä¸‰ç¨®è‚¡åƒ¹ï¼ˆä½¿ç”¨ PER çš„ç™¾åˆ†ä½æ•¸ï¼‰
        df_quarterly["é«˜è‚¡åƒ¹"] = df_quarterly["PER_æœ€é«˜å€¼"] * df_quarterly["æ¨ä¼°EPS"]
        df_quarterly["æ­£å¸¸è‚¡åƒ¹"] = df_quarterly["PER_å¹³å‡å€¼"] * df_quarterly["æ¨ä¼°EPS"]
        df_quarterly["ä½è‚¡åƒ¹"] = df_quarterly["PER_æœ€ä½å€¼"] * df_quarterly["æ¨ä¼°EPS"]

        # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        df_quarterly = df_quarterly.sort_values("date", ascending=False)

        # ç§»é™¤ç„¡æ•ˆçš„ä¼°å€¼
        df_quarterly = df_quarterly[
            df_quarterly[["ROE", "BVPS", "æ¨ä¼°EPS", "é«˜è‚¡åƒ¹", "æ­£å¸¸è‚¡åƒ¹", "ä½è‚¡åƒ¹"]].notna().all(axis=1)
        ]

        if df_quarterly.empty:
            logger.warning(f"è‚¡ç¥¨ {stock_id} ç„¡æœ‰æ•ˆçš„å­£åº¦æ•¸æ“š")
            return None

        # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„å­£åº¦æ•¸æ“š
        if len(df_quarterly) < 4:
            logger.warning(f"è‚¡ç¥¨ {stock_id} çš„å­£åº¦æ•¸æ“šä¸è¶³ 4 å­£")
            return None

        # æª¢æŸ¥æœ€æ–°æ•¸æ“šæ˜¯å¦åœ¨æœ€è¿‘ä¸€å¹´å…§
        latest_date = df_quarterly.iloc[0]["date"]
        one_year_ago = pd.Timestamp.now() - pd.DateOffset(years=1)
        
        if latest_date < one_year_ago:
            logger.warning(f"è‚¡ç¥¨ {stock_id} çš„æœ€æ–°æ•¸æ“šéæœŸï¼ˆ{latest_date.strftime('%Y-%m-%d')}ï¼‰")
            return None

        # åªè¿”å›æœ€è¿‘ 4 å­£çš„æ•¸æ“š
        df_quarterly = df_quarterly.head(4)

        return df_quarterly

    except Exception as e:
        logger.error(f"è™•ç†è‚¡ç¥¨ {stock_id} æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return None

# æ·»åŠ ç²å–å°è‚¡ä»£è™Ÿåˆ—è¡¨çš„å‡½æ•¸
def get_taiwan_stock_list():
    """å¾  FinMind API ç²å–å°è‚¡ä»£è™Ÿåˆ—è¡¨"""
    parameter = {
        "dataset": "TaiwanStockInfo",
        "token": FINMIND_API_KEY,
    }

    try:
        response = requests.get(FINMIND_URL, params=parameter)
        data = response.json()

        if "data" not in data or not isinstance(data["data"], list):
            logger.error("ç„¡æ³•å¾ FinMind API ç²å–è‚¡ç¥¨åˆ—è¡¨")
            return []

        # è½‰æ›ç‚º DataFrame
        df_stocks = pd.DataFrame(data["data"])
        
        # ç¢ºä¿ stock_id æ¬„ä½ç‚ºå­—ä¸²
        df_stocks["stock_id"] = df_stocks["stock_id"].astype(str)
        
        # éæ¿¾æ‰éä¸Šå¸‚è‚¡ç¥¨ï¼ˆé€šå¸¸è‚¡ç¥¨ä»£ç¢¼é•·åº¦ç‚º 4 ä½ï¼‰
        df_stocks = df_stocks[df_stocks["stock_id"].str.len() == 4]

        # éæ¿¾æ‰ç‰¹æ®Šè‚¡ç¥¨ï¼ˆå¦‚æ¬Šè­‰ã€æœŸè²¨ç­‰ï¼‰
        df_stocks = df_stocks[~df_stocks["stock_id"].str.startswith(('0', '9'))]

        
        # æ·»åŠ æ—¥èªŒè¨˜éŒ„
        logger.info(f"å¾ API ç²å–çš„è‚¡ç¥¨ç¸½æ•¸ï¼š{len(df_stocks)}")
        logger.info(f"è‚¡ç¥¨ä»£ç¢¼ç¯„åœï¼š{df_stocks['stock_id'].min()} åˆ° {df_stocks['stock_id'].max()}")
        
        stock_list = df_stocks["stock_id"].tolist()
        logger.info(f"æˆåŠŸç²å– {len(stock_list)} æ”¯ä¸Šå¸‚è‚¡ç¥¨")
        return stock_list
        
    except Exception as e:
        logger.error(f"ç²å–è‚¡ç¥¨åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return []

# æ·»åŠ æ–·é»çºŒå‚³ç›¸é—œè®Šé‡
progress_file = "recommend_v2_progress.json"

def save_progress(stock_list, current_index):
    """ä¿å­˜ä¸‹è¼‰é€²åº¦"""
    try:
        progress_data = {
            'stock_list': stock_list,
            'current_index': current_index,
            'timestamp': datetime.now().isoformat(),
            'total_stocks': len(stock_list)
        }
        with open(progress_file, 'w') as f:
            json.dump(progress_data, f)
        logger.info(f"å·²ä¿å­˜é€²åº¦ï¼šç•¶å‰è™•ç†åˆ°ç¬¬ {current_index} ç­†ï¼Œå…± {len(stock_list)} ç­†")
    except Exception as e:
        logger.error(f"ä¿å­˜ä¸‹è¼‰é€²åº¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

def load_progress():
    """è¼‰å…¥é€²åº¦"""
    try:
        if os.path.exists(progress_file):
            with open(progress_file, 'r') as f:
                data = json.load(f)
                # æª¢æŸ¥é€²åº¦æ˜¯å¦éæœŸï¼ˆè¶…é 24 å°æ™‚ï¼‰
                if datetime.fromisoformat(data['timestamp']) + timedelta(hours=24) < datetime.now():
                    logger.info("ä¸‹è¼‰é€²åº¦å·²éæœŸï¼Œå°‡é‡æ–°é–‹å§‹")
                    return None, 0
                
                # é©—è­‰æ•¸æ“šå®Œæ•´æ€§
                if 'stock_list' not in data or 'current_index' not in data or 'total_stocks' not in data:
                    logger.error("ä¸‹è¼‰é€²åº¦æª”æ¡ˆæ ¼å¼ä¸æ­£ç¢º")
                    return None, 0
                
                logger.info(f"è¼‰å…¥ä¸‹è¼‰é€²åº¦ï¼šå¾ç¬¬ {data['current_index']} ç­†é–‹å§‹ï¼Œå…± {data['total_stocks']} ç­†")
                return data['stock_list'], data['current_index']
    except Exception as e:
        logger.error(f"è¼‰å…¥ä¸‹è¼‰é€²åº¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    return None, 0

# æ·»åŠ å–æ¶ˆå‘½ä»¤
async def cancel_recommend(update: Update, context: CallbackContext) -> None:
    """å–æ¶ˆæ­£åœ¨åŸ·è¡Œçš„æ¨è–¦ä»»å‹™"""
    global is_processing, should_cancel
    
    if not is_processing:
        await update.message.reply_text("ç›®å‰æ²’æœ‰æ­£åœ¨åŸ·è¡Œçš„æ¨è–¦ä»»å‹™")
        return
    
    should_cancel = True
    is_processing = False
    await update.message.reply_text("å·²ç™¼é€å–æ¶ˆæŒ‡ä»¤ï¼Œæ­£åœ¨ç­‰å¾…ä»»å‹™çµæŸ...")

async def recommend_v2(update: Update, context: CallbackContext) -> None:
    """æ¨è–¦è‚¡ç¥¨ v2 ç‰ˆæœ¬"""
    global is_processing, should_cancel
    
    if is_processing:
        logger.warning("å·²æœ‰æ¨è–¦ä»»å‹™æ­£åœ¨åŸ·è¡Œä¸­")
        await update.message.reply_text("å·²æœ‰æ¨è–¦ä»»å‹™æ­£åœ¨åŸ·è¡Œä¸­ï¼Œè«‹ç¨å¾Œå†è©¦")
        return
    
    try:
        is_processing = True
        should_cancel = False
        logger.info("é–‹å§‹åŸ·è¡Œè‚¡ç¥¨æ¨è–¦ä»»å‹™")
        
        # è®€å– ROE æ•¸æ“š CSV æ–‡ä»¶
        csv_file = "stock_roe_data.csv"
        if not os.path.exists(csv_file):
            logger.error(f"æ‰¾ä¸åˆ° ROE æ•¸æ“šæ–‡ä»¶ï¼š{csv_file}")
            await update.message.reply_text("æ‰¾ä¸åˆ° ROE æ•¸æ“šæ–‡ä»¶ï¼Œè«‹å…ˆåŸ·è¡Œ /get_roe_data å‘½ä»¤")
            return
            
        try:
            # è®€å– ROE æ•¸æ“š
            logger.info("é–‹å§‹è®€å– ROE æ•¸æ“šæ–‡ä»¶")
            df_roe = pd.read_csv(csv_file)
            df_roe['stock_id'] = df_roe['stock_id'].astype(str)
            df_roe['date'] = pd.to_datetime(df_roe['date'])
            logger.info(f"æˆåŠŸè®€å– ROE æ•¸æ“šï¼Œå…± {len(df_roe)} ç­†è¨˜éŒ„")

            # è®€å–è‚¡åƒ¹æ•¸æ“š
            if not os.path.exists(STOCK_PRICE_FILE):
                logger.error(f"æ‰¾ä¸åˆ°è‚¡åƒ¹æ•¸æ“šæ–‡ä»¶ï¼š{STOCK_PRICE_FILE}")
                await update.message.reply_text("æ‰¾ä¸åˆ°è‚¡åƒ¹æ•¸æ“šæ–‡ä»¶ï¼Œè«‹å…ˆåŸ·è¡Œ /sync_stock_prices å‘½ä»¤")
                return

            logger.info("é–‹å§‹è®€å–è‚¡åƒ¹æ•¸æ“šæ–‡ä»¶")
            with open(STOCK_PRICE_FILE, 'r', encoding='utf-8') as f:
                stock_prices = json.load(f)
            logger.info(f"æˆåŠŸè®€å–è‚¡åƒ¹æ•¸æ“šï¼Œå…± {len(stock_prices)} æ”¯è‚¡ç¥¨")
            
        except Exception as e:
            logger.error(f"è®€å–æ•¸æ“šæ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
            await update.message.reply_text("è®€å–æ•¸æ“šæ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦")
            return

        # ç²å–æ‰€æœ‰è‚¡ç¥¨ä»£ç¢¼
        stock_list = df_roe['stock_id'].unique().tolist()
        logger.info(f"é–‹å§‹è™•ç†è‚¡ç¥¨ï¼Œç¸½å…± {len(stock_list)} æ”¯è‚¡ç¥¨")

        # è™•ç†æ¯æ”¯è‚¡ç¥¨
        all_results = []
        total_stocks = len(stock_list)
        processed_count = 0
        filtered_count = 0
        no_quarter_data_count = 0
        
        for stock_id in stock_list:
            if should_cancel:
                logger.info("æ”¶åˆ°å–æ¶ˆæŒ‡ä»¤ï¼Œåœæ­¢è™•ç†")
                await update.message.reply_text("ä»»å‹™å·²å–æ¶ˆ")
                break
                
            try:
                # ç²å–è©²è‚¡ç¥¨çš„æ‰€æœ‰æ•¸æ“š
                stock_data = df_roe[df_roe['stock_id'] == stock_id].copy()
                if stock_data.empty:
                    logger.debug(f"è‚¡ç¥¨ {stock_id} ç„¡ ROE æ•¸æ“šï¼Œè·³é")
                    continue

                # ç¢ºä¿æ•¸æ“šæŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
                stock_data = stock_data.sort_values('date', ascending=False)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰è¿‘å››å­£æ•¸æ“š
                if len(stock_data) < 4:
                    logger.debug(f"è‚¡ç¥¨ {stock_id} çš„å­£åº¦æ•¸æ“šä¸è¶³ 4 å­£ï¼Œè·³é")
                    no_quarter_data_count += 1
                    continue

                # å–æœ€è¿‘ 4 å­£æ•¸æ“š
                recent_data = stock_data.head(4).copy()
                
                # ä½¿ç”¨ .loc è¨ˆç®— ROEï¼Œä¸¦æ·»åŠ æ•¸å€¼æª¢æŸ¥
                recent_data.loc[:, 'ROE'] = np.where(
                    (recent_data['PER'] != 0) & (recent_data['PER'].notna()) & (recent_data['PBR'].notna()),
                    (recent_data['PBR'] / recent_data['PER']) * 100,
                    np.nan
                )
                
                # æª¢æŸ¥ ROE æ˜¯å¦æœ‰æ•ˆ
                if recent_data['ROE'].isna().all():
                    logger.debug(f"è‚¡ç¥¨ {stock_id} çš„ ROE è¨ˆç®—çµæœå…¨ç‚ºç„¡æ•ˆå€¼ï¼Œè·³é")
                    continue
                
                # æª¢æŸ¥ ROE æ˜¯å¦å¤§æ–¼ 15
                latest_roe = recent_data['ROE'].iloc[0]
                if pd.isna(latest_roe) or latest_roe <= 15:
                    logger.debug(f"è‚¡ç¥¨ {stock_id} çš„ ROE ({latest_roe if not pd.isna(latest_roe) else 'NA'}%) ç„¡æ•ˆæˆ–ä½æ–¼ 15%ï¼Œè·³é")
                    continue

                # æª¢æŸ¥ PER æ˜¯å¦ç‚º 0
                latest_per = recent_data['PER'].iloc[0]
                if pd.isna(latest_per) or latest_per == 0:
                    logger.debug(f"è‚¡ç¥¨ {stock_id} çš„ PER ({latest_per if not pd.isna(latest_per) else 'NA'}) ç„¡æ•ˆæˆ–ç‚º 0ï¼Œè·³é")
                    continue

                # æª¢æŸ¥æœ€è¿‘å››å­£æ˜¯å¦æœ‰ä»»ä½•ä¸€å­£çš„ PER æˆ– ROE ç‚º 0
                if (recent_data['PER'] == 0).any() or (recent_data['ROE'] == 0).any():
                    logger.debug(f"è‚¡ç¥¨ {stock_id} çš„æœ€è¿‘å››å­£ä¸­æœ‰ PER æˆ– ROE ç‚º 0 çš„æ•¸æ“šï¼Œè·³é")
                    continue

                # è¨ˆç®— ROE è¶¨å‹¢
                roe_values = recent_data['ROE'].dropna().tolist()
                valid_roe_values = [roe for roe in roe_values if not pd.isna(roe) and roe > 0]
                if len(valid_roe_values) < 4:
                    logger.debug(f"è‚¡ç¥¨ {stock_id} çš„æœ‰æ•ˆ ROE æ•¸æ“šä¸è¶³ 4 å­£ï¼Œè·³é")
                    continue
                
                # æª¢æŸ¥ ROE æ˜¯å¦ç©©å®šå‘ä¸Šå‡
                roe_trend = all(valid_roe_values[i] <= valid_roe_values[i+1] for i in range(len(valid_roe_values)-1))
                roe_volatility = (max(valid_roe_values) - min(valid_roe_values)) / min(valid_roe_values) * 100 if min(valid_roe_values) > 0 else float('inf')
                
                # è¦æ±‚ ROE ç©©å®šå‘ä¸Šå‡ä¸”æ³¢å‹•ç‡ä¸è¶…é 20%
                if not roe_trend or roe_volatility > 20:
                    logger.debug(f"è‚¡ç¥¨ {stock_id} çš„ ROE è¶¨å‹¢ä¸ç¬¦åˆè¦æ±‚ï¼ˆè¶¨å‹¢ï¼š{'ä¸Šå‡' if roe_trend else 'ä¸‹é™'}, æ³¢å‹•ç‡ï¼š{roe_volatility:.2f}%ï¼‰ï¼Œè·³é")
                    continue

                # ç²å–ç•¶å‰è‚¡åƒ¹
                current_price = stock_prices.get(stock_id)
                if current_price is None or current_price <= 0:
                    logger.debug(f"è‚¡ç¥¨ {stock_id} ç„¡ç•¶å‰è‚¡åƒ¹æ•¸æ“šæˆ–è‚¡åƒ¹ç„¡æ•ˆï¼Œè·³é")
                    continue

                # è¨ˆç®— BVPS å’Œæ¨ä¼° EPS
                latest_data = recent_data.iloc[0]
                if pd.isna(latest_data['PBR']) or latest_data['PBR'] <= 0:
                    logger.debug(f"è‚¡ç¥¨ {stock_id} çš„ PBR ç„¡æ•ˆï¼Œè·³é")
                    continue
                
                bvps = current_price / latest_data['PBR']
                estimated_eps = (latest_roe / 100) * bvps if not pd.isna(latest_roe) else 0

                # è¨ˆç®—ä¸‰ç¨®è‚¡åƒ¹ï¼ˆæ·»åŠ æ•¸å€¼æª¢æŸ¥ï¼‰
                if pd.isna(latest_data['PER']) or latest_data['PER'] <= 0 or pd.isna(estimated_eps) or estimated_eps <= 0:
                    logger.debug(f"è‚¡ç¥¨ {stock_id} çš„ PER æˆ– EPS ç„¡æ•ˆï¼Œè·³é")
                    continue

                low_price = latest_data['PER'] * 0.8 * estimated_eps
                normal_price = latest_data['PER'] * estimated_eps
                high_price = latest_data['PER'] * 1.2 * estimated_eps

                # è¨ˆç®—åƒ¹å€¼åˆ†æ•¸ï¼ˆæ·»åŠ æ•¸å€¼æª¢æŸ¥ï¼‰
                if low_price <= 0:
                    logger.debug(f"è‚¡ç¥¨ {stock_id} çš„ä½ä¼°åƒ¹æ ¼è¨ˆç®—çµæœç„¡æ•ˆï¼Œè·³é")
                    continue

                price_to_low = current_price / low_price
                value_score = (price_to_low * 0.7 + (1 / latest_data['PER']) * 0.3) * 100
                
                result = {
                    "stock_id": stock_id,
                    "current_price": current_price,
                    "value_score": value_score,
                    "roe": latest_roe,
                    "price_to_low": price_to_low,
                    "current_per": latest_data['PER'],
                    "roe_trend": roe_trend,
                    "roe_volatility": roe_volatility,
                    "ä½è‚¡åƒ¹": low_price,
                    "æ­£å¸¸è‚¡åƒ¹": normal_price,
                    "é«˜è‚¡åƒ¹": high_price,
                    "æ¨ä¼°EPS": estimated_eps
                }
                
                all_results.append(result)
                filtered_count += 1
                logger.debug(f"è‚¡ç¥¨ {stock_id} ç¬¦åˆç¯©é¸æ¢ä»¶ï¼ˆROE: {latest_roe:.2f}%, åƒ¹å€¼åˆ†æ•¸: {value_score:.2f}ï¼‰")

                processed_count += 1
                # æ¯è™•ç† 100 æ”¯è‚¡ç¥¨ç™¼é€ä¸€æ¬¡é€²åº¦æ›´æ–°
                if processed_count % 100 == 0:
                    progress = (processed_count / total_stocks) * 100
                    logger.info(f"è™•ç†é€²åº¦ï¼š{progress:.1f}%ï¼Œå·²è™•ç† {processed_count} æ”¯è‚¡ç¥¨ï¼Œç¬¦åˆæ¢ä»¶ {filtered_count} æ”¯ï¼Œç„¡å››å­£è³‡æ–™ {no_quarter_data_count} æ”¯")
                    await update.message.reply_text(f"è™•ç†é€²åº¦ï¼š{progress:.1f}% ({processed_count}/{total_stocks})\nç¬¦åˆæ¢ä»¶ï¼š{filtered_count} æ”¯\nç„¡å››å­£è³‡æ–™ï¼š{no_quarter_data_count} æ”¯")
                
            except Exception as e:
                logger.error(f"è™•ç†è‚¡ç¥¨ {stock_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
                continue
        
        if not all_results:
            logger.warning("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨")
            await update.message.reply_text("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨")
            return

        logger.info(f"ç¯©é¸å®Œæˆï¼Œå…±æœ‰ {len(all_results)} æ”¯è‚¡ç¥¨ç¬¦åˆæ¢ä»¶ï¼Œ{no_quarter_data_count} æ”¯è‚¡ç¥¨ç„¡å››å­£è³‡æ–™")

        # æ ¹æ“šåƒ¹å€¼åˆ†æ•¸æ’åº
        all_results.sort(key=lambda x: x["value_score"], reverse=True)
        
        # é¸å–å‰ 10 æ”¯è‚¡ç¥¨
        top_10 = all_results[:10]
        logger.info("å‰ 10 åè‚¡ç¥¨ï¼ˆä¾åƒ¹å€¼åˆ†æ•¸æ’åºï¼‰ï¼š" + ", ".join([f"{stock['stock_id']}({stock['value_score']:.2f})" for stock in top_10]))
        
        # ç”Ÿæˆæ¨è–¦è¨Šæ¯
        message = "ğŸ“Š è‚¡ç¥¨æ¨è–¦ (v2)\n\n"
        message += f"ğŸ”¹ è™•ç†çµ±è¨ˆï¼š\n"
        message += f"- ç¸½è‚¡ç¥¨æ•¸ï¼š{total_stocks} æ”¯\n"
        message += f"- ç„¡å››å­£è³‡æ–™ï¼š{no_quarter_data_count} æ”¯\n"
        message += f"- ç¬¦åˆæ¢ä»¶ï¼š{filtered_count} æ”¯\n\n"
        message += "ğŸ”¹ æ ¹æ“šåƒ¹å€¼åˆ†æ•¸æ’åºï¼š\n"
        for i, stock in enumerate(top_10, 1):
            message += f"{i}. {stock['stock_id']}\n"
            message += f"   ç•¶å‰è‚¡åƒ¹: {stock['current_price']:.2f}\n"
            message += f"   åƒ¹å€¼åˆ†æ•¸: {stock['value_score']:.2f}\n"
            message += f"   ROE: {stock['roe']:.2f}%\n"
            message += f"   æ¨ä¼°EPS: {stock['æ¨ä¼°EPS']:.2f}\n"
            message += f"   ä½è‚¡åƒ¹: {stock['ä½è‚¡åƒ¹']:.2f}\n"
            message += f"   æ­£å¸¸è‚¡åƒ¹: {stock['æ­£å¸¸è‚¡åƒ¹']:.2f}\n"
            message += f"   é«˜è‚¡åƒ¹: {stock['é«˜è‚¡åƒ¹']:.2f}\n"
            message += f"   æœ¬ç›Šæ¯”: {stock['current_per']:.2f}\n"
            message += f"   ROEè¶¨å‹¢: {'ä¸Šå‡' if stock['roe_trend'] else 'ä¸‹é™'}\n"
            message += f"   ROEæ³¢å‹•ç‡: {stock['roe_volatility']:.2f}%\n\n"
        
        logger.info("å®Œæˆæ¨è–¦è¨Šæ¯ç”Ÿæˆï¼Œæº–å‚™ç™¼é€")
        await update.message.reply_text(message)
        logger.info("æ¨è–¦è¨Šæ¯å·²ç™¼é€")
        
    except Exception as e:
        logger.error(f"æ¨è–¦è‚¡ç¥¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
        await update.message.reply_text("è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦")
    finally:
        is_processing = False
        logger.info("æ¨è–¦ä»»å‹™çµæŸ")

# æ·»åŠ æ–°çš„å¸¸é‡
ROE_DATA_FILE = "stock_roe_data.json"

async def get_stock_roe_data(update: Update, context: CallbackContext) -> None:
    """è·å–æ‰€æœ‰å°è‚¡çš„ ROE æ•°æ®å¹¶ä¿å­˜ç‚º CSV"""
    try:
        # è·å–æ‰€æœ‰å°è‚¡ä»£ç 
        stock_list = get_taiwan_stock_list()
        if not stock_list:
            await update.message.reply_text("ç„¡æ³•ç²å–å°è‚¡åˆ—è¡¨ï¼Œè«‹ç¨å¾Œå†è©¦")
            return
            
        logger.info(f"æˆåŠŸç²å– {len(stock_list)} æ”¯å°è‚¡ä»£ç¢¼")

        # åˆ›å»º CSV æ–‡ä»¶
        csv_file = "stock_roe_data.csv"
        no_data_file = "no_data_stocks.json"  # è®°å½•æ²¡æœ‰æ•°æ®çš„è‚¡ç¥¨
        processed_count = 0
        existing_stocks = set()
        no_data_stocks = set()

        # è¯»å–æ²¡æœ‰æ•°æ®çš„è‚¡ç¥¨åˆ—è¡¨
        if os.path.exists(no_data_file):
            try:
                with open(no_data_file, 'r', encoding='utf-8') as f:
                    no_data_stocks = set(json.load(f))
                    logger.info(f"è®€å–åˆ° {len(no_data_stocks)} æ”¯æ²’æœ‰æ•¸æ“šçš„è‚¡ç¥¨")
            except Exception as e:
                logger.error(f"è®€å–ç„¡æ•¸æ“šè‚¡ç¥¨åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                no_data_stocks = set()

        # æ£€æŸ¥ç°æœ‰çš„ CSV æ–‡ä»¶
        if os.path.exists(csv_file):
            try:
                df_existing = pd.read_csv(csv_file)
                # ç¡®ä¿ stock_id ä¸ºå­—ç¬¦ä¸²ç±»å‹
                df_existing['stock_id'] = df_existing['stock_id'].astype(str)
                existing_stocks = set(df_existing['stock_id'].unique())
                logger.info(f"ç¾æœ‰ CSV æ–‡ä»¶ä¸­å·²æœ‰ {len(existing_stocks)} æ”¯è‚¡ç¥¨çš„æ•¸æ“š")
                logger.info(f"CSV ä¸­çš„è‚¡ç¥¨ç¤ºä¾‹ï¼š{list(existing_stocks)[:5]}")
            except Exception as e:
                logger.error(f"è®€å–ç¾æœ‰ CSV æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                existing_stocks = set()

        logger.info(f"æœ¬æ¬¡å°‡è™•ç†å‰ {len(stock_list)} æ”¯è‚¡ç¥¨")

        # è·å–éœ€è¦å¤„ç†çš„è‚¡ç¥¨åˆ—è¡¨ï¼ˆæ’é™¤å·²æœ‰æ•°æ®çš„è‚¡ç¥¨ï¼‰
        missing_stocks = [stock_id for stock_id in stock_list 
                        if stock_id not in existing_stocks]
        logger.info(f"å…¶ä¸­ {len(missing_stocks)} æ”¯è‚¡ç¥¨éœ€è¦è™•ç†")

        if not missing_stocks:
            await update.message.reply_text("æ‰€æœ‰è‚¡ç¥¨æ•¸æ“šéƒ½å·²æ˜¯æœ€æ–°")
            return

        # å¤„ç†ç¼ºå¤±çš„è‚¡ç¥¨
        new_no_data_stocks = set()  # è®°å½•æœ¬æ¬¡æ‰§è¡Œä¸­å‘ç°æ²¡æœ‰æ•°æ®çš„è‚¡ç¥¨
        for stock_id in missing_stocks:
            try:
                logger.info(f"æ­£åœ¨æŸ¥è©¢è‚¡ç¥¨ {stock_id} çš„ ROE æ•¸æ“š...")
                # è®¾ç½® API å‚æ•°
                parameter = {
                    "dataset": "TaiwanStockPER",
                    "data_id": stock_id,
                    "start_date": "2020-01-01",
                    "end_date": datetime.now().strftime('%Y-%m-%d'),
                    "token": FINMIND_API_KEY,
                }

                # è·å– ROE æ•°æ®
                async with aiohttp.ClientSession() as session:
                    async with session.get(FINMIND_URL, params=parameter) as response:
                        if response.status != 200:
                            logger.error(f"API è«‹æ±‚å¤±æ•—ï¼Œè‚¡ç¥¨ {stock_id}ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status}")
                            new_no_data_stocks.add(stock_id)
                            continue

                        data = await response.json()
                        if "data" not in data or not isinstance(data["data"], list) or len(data["data"]) == 0:
                            logger.warning(f"è‚¡ç¥¨ {stock_id} æ²’æœ‰æ•¸æ“š")
                            new_no_data_stocks.add(stock_id)
                            continue

                        logger.info(f"æˆåŠŸç²å–è‚¡ç¥¨ {stock_id} çš„ ROE æ•¸æ“š")

                        # è½¬æ¢æ•°æ®ä¸º DataFrame
                        df = pd.DataFrame(data["data"])
                        
                        # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
                        df["date"] = pd.to_datetime(df["date"])
                        
                        # ç¡®ä¿æ•°å€¼å­—æ®µä¸ºæ•°å€¼ç±»å‹
                        numeric_columns = ["PER", "PBR", "ROE"]
                        for col in numeric_columns:
                            if col in df.columns:
                                df[col] = pd.to_numeric(df[col], errors="coerce")

                        # æ·»åŠ è‚¡ç¥¨ä»£ç åˆ—
                        df["stock_id"] = stock_id

                        # å°†æ•°æ®å†™å…¥ CSV
                        if processed_count == 0 and not os.path.exists(csv_file):
                            # ç¬¬ä¸€æ¬¡å†™å…¥ï¼Œåˆ›å»ºæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
                            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                        else:
                            # åç»­å†™å…¥ï¼Œè¿½åŠ æ•°æ®ï¼ˆä¸åŒ…å«è¡¨å¤´ï¼‰
                            df.to_csv(csv_file, mode='a', header=False, index=False, encoding='utf-8-sig')

                        processed_count += 1

                        # æ¯å¤„ç† 10 æ”¯è‚¡ç¥¨å‘é€ä¸€æ¬¡è¿›åº¦æ›´æ–°
                        if processed_count % 10 == 0:
                            logger.info(f"å·²è™•ç† {processed_count}/{len(missing_stocks)} æ”¯è‚¡ç¥¨")
                            await update.message.reply_text(f"å·²è™•ç† {processed_count}/{len(missing_stocks)} æ”¯è‚¡ç¥¨")

                        # ç­‰å¾… 0.5 ç§’å†å¤„ç†ä¸‹ä¸€æ”¯è‚¡ç¥¨
                        await asyncio.sleep(0.5)

            except Exception as e:
                logger.error(f"è™•ç†è‚¡ç¥¨ {stock_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                new_no_data_stocks.add(stock_id)
                continue

        # æ›´æ–°æ²¡æœ‰æ•°æ®çš„è‚¡ç¥¨åˆ—è¡¨
        if new_no_data_stocks:
            no_data_stocks.update(new_no_data_stocks)
            with open(no_data_file, 'w', encoding='utf-8') as f:
                json.dump(list(no_data_stocks), f, ensure_ascii=False, indent=2)
            logger.info(f"æ–°å¢ {len(new_no_data_stocks)} æ”¯æ²’æœ‰æ•¸æ“šçš„è‚¡ç¥¨åˆ°è¨˜éŒ„ä¸­")

        await update.message.reply_text(f"å®Œæˆï¼å…±è™•ç† {processed_count} æ”¯è‚¡ç¥¨æ•¸æ“š")

    except Exception as e:
        logger.error(f"ç²å–è‚¡ç¥¨ ROE æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        await update.message.reply_text("è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦")

# æ·»åŠ æ–°çš„å¸¸é‡
STOCK_PRICE_FILE = "stock_prices.json"

async def sync_stock_prices(update: Update, context: CallbackContext) -> None:
    """åŒæ­¥æ‰€æœ‰è‚¡ç¥¨çš„æœ€æ–°åƒ¹æ ¼ä¸¦ä¿å­˜åˆ° JSON æ–‡ä»¶"""
    try:
        # ä½¿ç”¨ get_taiwan_stock_list() ç²å–è‚¡ç¥¨åˆ—è¡¨
        stock_list = get_taiwan_stock_list()
        if not stock_list:
            await update.message.reply_text("ç„¡æ³•ç²å–è‚¡ç¥¨åˆ—è¡¨ï¼Œè«‹ç¨å¾Œå†è©¦")
            return

        logger.info(f"éœ€è¦æ›´æ–° {len(stock_list)} æ”¯è‚¡ç¥¨çš„åƒ¹æ ¼")

        # ç™¼é€é–‹å§‹æ›´æ–°çš„è¨Šæ¯
        status_message = await update.message.reply_text("é–‹å§‹æ›´æ–°è‚¡ç¥¨åƒ¹æ ¼...")
        processed_count = 0
        updated_prices = {}

        # è™•ç†æ¯æ”¯è‚¡ç¥¨
        for stock_id in stock_list:
            try:
                # ä½¿ç”¨ get_current_stock_price ç²å–åƒ¹æ ¼
                current_price = await get_current_stock_price(stock_id)
                if current_price is not None:
                    updated_prices[stock_id] = current_price
                    processed_count += 1

                # æ¯è™•ç† 100 æ”¯è‚¡ç¥¨æ›´æ–°ä¸€æ¬¡é€²åº¦
                if processed_count % 100 == 0:
                    progress = (processed_count / len(stock_list)) * 100
                    await status_message.edit_text(f"æ›´æ–°é€²åº¦ï¼š{progress:.1f}% ({processed_count}/{len(stock_list)})")

                # æ¯æ”¯è‚¡ç¥¨è™•ç†å®Œå¾Œæš«åœä¸€ä¸‹
                await asyncio.sleep(0.1)

            except Exception as e:
                logger.error(f"è™•ç†è‚¡ç¥¨ {stock_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue

        # ä¿å­˜æ›´æ–°å¾Œçš„åƒ¹æ ¼æ•¸æ“š
        with open(STOCK_PRICE_FILE, 'w', encoding='utf-8') as f:
            json.dump(updated_prices, f, ensure_ascii=False, indent=2)

        # ç™¼é€å®Œæˆè¨Šæ¯
        await update.message.reply_text(f"è‚¡ç¥¨åƒ¹æ ¼æ›´æ–°å®Œæˆï¼å…±æ›´æ–° {len(updated_prices)} æ”¯è‚¡ç¥¨çš„åƒ¹æ ¼")

    except Exception as e:
        logger.error(f"åŒæ­¥è‚¡ç¥¨åƒ¹æ ¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        await update.message.reply_text("è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦")

def main():
    global app
    load_dotenv()
    
    BOT_TOKEN = os.getenv("BOT_TOKEN")
    if not BOT_TOKEN:
        raise ValueError("æœªæ‰¾åˆ° BOT_TOKENï¼Œè«‹åœ¨ Heroku ç’°å¢ƒè®Šæ•¸è¨­å®š BOT_TOKEN")
    
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)
    
    try:
        app = Application.builder().token(BOT_TOKEN).build()

        app.add_handler(CommandHandler("start", start))
        app.add_handler(CommandHandler("get_roe_data", get_stock_roe_data))
        app.add_handler(CommandHandler("recommend_v2", recommend_v2))
        app.add_handler(CommandHandler("cancel_recommend", cancel_recommend))
        app.add_handler(CommandHandler("etf", etf))
        app.add_handler(CommandHandler("stock_estimate", stock_estimate))
        app.add_handler(CommandHandler("sync_stock_prices", sync_stock_prices))

        logger.info("Bot å·²å•Ÿå‹•ä¸¦é–‹å§‹é‹è¡Œ...")
        app.run_polling()
        
    except Exception as e:
        logger.error(f"é‹è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        if app:
            app.stop()
        sys.exit(1)

if __name__ == "__main__":
    main()
